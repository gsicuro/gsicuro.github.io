<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gabriele Sicuro</title>
    <link>https://gsicuro.github.io/</link>
      <atom:link href="https://gsicuro.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Gabriele Sicuro</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gsicuro.github.io/media/icon_hu4c465b94eb6b23f2c7fc783ea01b5261_1018239_512x512_fill_lanczos_center_3.png</url>
      <title>Gabriele Sicuro</title>
      <link>https://gsicuro.github.io/</link>
    </image>
    
    <item>
      <title>Spazi vettoriali</title>
      <link>https://gsicuro.github.io/lectures/im2on/ch1/sec1/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/im2on/ch1/sec1/</guid>
      <description>&lt;h3 id=&#34;vettori-applicati-e-vettori-geometrici&#34;&gt;Vettori applicati e vettori geometrici&lt;/h3&gt;
&lt;p&gt;Per introdurre il concetto di &lt;em&gt;spazio vettoriale&lt;/em&gt; iniziamo da un
contesto più concreto e forse più semplice. Consideriamo due punti $A$ e
$B$ nello spazio &lt;em&gt;ordinario&lt;/em&gt;. Un &lt;em&gt;vettore applicato
${\boldsymbol v}=\overrightarrow{AB}$ in $A$ con punto finale $B$&lt;/em&gt; può
essere immaginato come una freccia che collega $A$ e $B$,&lt;/p&gt;
&lt;img src=&#34;https://gsicuro.github.io/images/im2/main-0.jpg&#34; alt=&#34;vettore&#34; style=&#34;width:20%;&#34;/&gt; 
&lt;p&gt;Un vettore applicato è caratterizzato dal suo punto di applicazione,
$A$, da una direzione, un verso e una lunghezza. Se $A=B$, allora il
corrispondente vettore è detto &lt;em&gt;nullo&lt;/em&gt; e viene indicato con $\mathbf 0$:
la sua lunghezza è zero e la sua direzione e il suo verso non sono
definiti. Due vettori ${\boldsymbol v}=\overrightarrow{AB}$ e
${\boldsymbol u}=\overrightarrow{CD}$ sono &lt;em&gt;equipollenti&lt;/em&gt; se hanno
stessa direzione, stessa lunghezza e stesso verso. L&amp;rsquo;equipollenza è una
relazione di equivalenza e un &lt;em&gt;vettore geometrico&lt;/em&gt; corrisponde ad una
classe di equivalenza secondo equipollenza. Uno specifico vettore
applicato è perciò un &lt;em&gt;rappresentante&lt;/em&gt; di questa classe di equivalenza,
mentre tutti i vettori con stessa direzione, stessa lunghezza e stesso
verso corrispondono allo stesso oggetto di tipo &amp;ldquo;vettore geometrico&amp;rdquo;.&lt;/p&gt;
&lt;h4 id=&#34;operazioni-tra-vettori-geometrici&#34;&gt;Operazioni tra vettori geometrici&lt;/h4&gt;
&lt;p&gt;I vettori geometrici sono oggetti molto diversi dai numeri usuali, ma è
ugualmente possibile introdurre una serie di operazioni tra di essi.&lt;/p&gt;
&lt;p&gt;Dati due vettori geometrici ${\boldsymbol v}$ e ${\boldsymbol u}$, la
loro &lt;em&gt;somma&lt;/em&gt; ${\boldsymbol u}+{\boldsymbol v}$ può essere definita
concatenando due loro rappresentanti. Per esempio, scelto un punto $A$,
si applica prima ${\boldsymbol v}$ ad $A$ si ottiene il vettore
$\overrightarrow{AB}$ con coda in un punto $B$, e, consecutivamente,
applicando ${\boldsymbol u}$ a $B$ si ottiene il vettore
$\overrightarrow{BC}$ con coda in $C$. Possiamo quindi &lt;em&gt;definire&lt;/em&gt;
$\overrightarrow{AC}$ come rappresentante del vettore geometrico somma
${\boldsymbol v}+{\boldsymbol u}$. La costruzione ora descritta
corrisponde ala cosiddetta &lt;em&gt;regola del parallelogramma&lt;/em&gt;, ed è
equivalente a considerare ${\boldsymbol u}$ e ${\boldsymbol v}$
applicati allo stesso punto $A$ e costrure su di essi un
parallelogramma, considerando quindi come somma la diagonale del
quadrilatero ottenuto:&lt;/p&gt;
&lt;img src=&#34;https://gsicuro.github.io/images/im2/main-1.jpg&#34; alt=&#34;Regola del parallelogramma&#34; style=&#34;width:40%;&#34;/&gt; 
&lt;p&gt;È evidente dalla costruzione stessa che l&amp;rsquo;operazione è commutativa,
ovvero
${\boldsymbol v}+{\boldsymbol u}={\boldsymbol u}+{\boldsymbol v}$, e
tale per cui ${\boldsymbol v}+\mathbf 0={\boldsymbol v}$. L&amp;rsquo;operazione
di somma che abbiamo definito è anche &lt;em&gt;associativa&lt;/em&gt;. Se abbiamo tre
vettori ${\boldsymbol v}$, ${\boldsymbol u}$ e ${\boldsymbol w}$, allora&lt;/p&gt;
&lt;img src=&#34;https://gsicuro.github.io/images/im2/somma.png&#34; alt=&#34;Associatività&#34; style=&#34;width:80%;&#34;/&gt; 
&lt;p&gt;Infine, dato un vettore applicato $\overrightarrow{AB}$, il vettore
applicato $\overrightarrow{BA}$ è tale per cui
$\overrightarrow{AB}+\overrightarrow{BA}=\mathbf 0$: se il vettore
geometrico associato a $\overrightarrow{AB}$ è ${\boldsymbol v}$,
denotiamo quindi $-{\boldsymbol v}$ il vettore geometrico associato a
$\overrightarrow{BA}$ e scriviamo
${\boldsymbol v}+(-{\boldsymbol v})=\mathbf 0$: $-{\boldsymbol v}$ è
l&amp;rsquo;&lt;em&gt;opposto&lt;/em&gt; di ${\boldsymbol v}$.&lt;/p&gt;
&lt;p&gt;È possibile anche introdurre l&amp;rsquo;operazione &lt;em&gt;moltiplicazione per uno
scalare $c\in{\mathbb R}$&lt;/em&gt;: il vettore $c{\boldsymbol v}$ ha stessa
direzione di ${\boldsymbol v}$, stesso verso se $c&amp;gt;0$ e verso opposto se
$c&amp;lt;0$, e lunghezza uguale a $|c|$ volte quella di ${\boldsymbol v}$.&lt;/p&gt;
&lt;p&gt;Se $c=0$, allora $c{\boldsymbol v}=\mathbf 0$. È facile verificare che,
se $a,b\in{\mathbb R}$, allora
$(a+b){\boldsymbol v}=a{\boldsymbol v}+b{\boldsymbol v}$,
$(ab){\boldsymbol v}=a(b{\boldsymbol v})$ e, dati due vettori
${\boldsymbol v}$ e ${\boldsymbol u}$,
$a({\boldsymbol v}+{\boldsymbol u})=a{\boldsymbol v}+a{\boldsymbol u}$.&lt;/p&gt;
&lt;h3 id=&#34;spazi-vettoriali&#34;&gt;Spazi vettoriali&lt;/h3&gt;
&lt;p&gt;Lo spazio dei vettori geometrici può avere una interpretazione intuitiva
abbastanza semplice. Può rappresentare, per esempio, lo spazio degli
&lt;em&gt;spostamenti&lt;/em&gt; nel piano. Tuttavia la sua struttura può essere
generalizzata in forma astratta: essa è caratterizzata dalla presenza di
due operazioni binarie (somma tra vettori e prodotto per scalare) e lo
&lt;em&gt;spazio vettoriale&lt;/em&gt; dei vettori geometrici è stato introdotto proprio
per mezzo di queste operazioni.&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;idea ora è tentare di caratterizzare cosa sia uno spazio vettoriale
senza necessariamente avere in mente una rappresentazione pittorica. In
generale, la costruzione di uno spazio vettoriale richiede l&amp;rsquo;esistenza
di un insieme $\mathbb V$ di &amp;ldquo;vettori&amp;rdquo; e di un altro insieme di
&amp;ldquo;scalari&amp;rdquo;, ${\mathbb K}$. Ci limiteremo ad assumere che ${\mathbb K}$
sia, a seconda dei casi ${\mathbb Q}$, ${\mathbb R}$ o ${\mathbb C}$, ma
utilizzeremo ${\mathbb K}$ per lavorare in maggiore generalità.&lt;/p&gt;
&lt;p&gt;Il concetto di spazio vettoriale può quindi essere formalizzato come
segue.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Definizione &lt;/b&gt;
Uno spazio vettoriale su un campo
${\mathbb K}$ è un insieme non vuoto $\mathbb V$ tale per cui esistono
due operazioni binarie, ovvero
$+\colon\mathbb V\times\mathbb V\to\mathbb V$, detta &lt;i&gt;somma&lt;/i&gt;, e
$\cdot \colon{\mathbb K}\times\mathbb V\to\mathbb V$, detta &lt;i&gt;prodotto
per uno scalare&lt;/i&gt;, con le seguenti proprietà:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;la somma $+$ è commutativa e associativa; esiste un elemento neutro
$\mathbf 0$, ovvero se ${\boldsymbol v}\in\mathbb V$, allora
${\boldsymbol v}+\mathbf 0=\mathbf 0+{\boldsymbol v}={\boldsymbol v}$;
esiste l&amp;rsquo;elemento opposto, ovvero se ${\boldsymbol v}\in\mathbb V$
esiste $-{\boldsymbol v}\in\mathbb V$ tale per cui
${\boldsymbol v}+(-{\boldsymbol v})=\mathbf 0$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;il prodotto $\cdot$ è distributivo sulla somma di vettori (ovvero
$a({\boldsymbol v}+{\boldsymbol u})=a{\boldsymbol v}+a{\boldsymbol u}$
per $a\in{\mathbb K}$ e
${\boldsymbol v},{\boldsymbol u}\in\mathbb V$) e sulla somma di
scalari (ovvero
$(a+b){\boldsymbol v}=a{\boldsymbol v}+b{\boldsymbol v}$ per
$a,b\in{\mathbb K}$ e ${\boldsymbol v}\mathbb V$); infine l&amp;rsquo;elemento
neutro $1$ di ${\mathbb K}$ è tale per cui
$1{\boldsymbol v}={\boldsymbol v}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;La definizione precedente si applica in effetti ai vettori geometrici,
ed è anzi ispirata da essi. Come potete notare, è richiesto che
${\mathbb K}$ sia un &lt;a href=&#34;https://it.wikipedia.org/wiki/Campo_%28matematica%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;campo&lt;/a&gt;: come anticipato, per semplicità assumeremo
che ${\mathbb K}$ sia un campo numerico come ${\mathbb R}$,
${\mathbb Q}$ e ${\mathbb C}$.&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34; role=&#34;alert&#34;&gt;
Un esempio di spazio vettoriale è dato dai vettori nel piano
$\mathbb V={\mathbb R}^2$. Un vettore nel piano ${\boldsymbol x}$ è
rappresentato da una freccia applicata nell&#39;origine che punta in una
posizione del piano
&lt;img src=&#34;https://gsicuro.github.io/images/im2/main-5.jpg&#34; alt=&#34;Vettore nel piano&#34; style=&#34;width:50%;&#34;/&gt; 
&lt;p&gt;e può rappresentarsi come una coppia di numeri come
$${\boldsymbol x}=
\begin{pmatrix}
x_1\\ x_2
\end{pmatrix}
\in{\mathbb R}^2,$$
corrispondenti alle sue coordinate. Dati due vettori ${\boldsymbol x}_1$
e ${\boldsymbol x}_2$, una loro combinazione lineare con coefficienti
reali $c_1$ e $c_2$ si ottiene come
$${\boldsymbol x}_1=
\begin{pmatrix}
x_{11}\\x_{12}
\end{pmatrix}
,\quad  {\boldsymbol x}_2=
\begin{pmatrix}
x_{21}\\x_{22}
\end{pmatrix}
\Rightarrow c_1{\boldsymbol x}_1+c_2{\boldsymbol x}_2=
\begin{pmatrix}
c_1x_{11}+c_2x_{21}\\c_1x_{12}+c_2x_{22}
\end{pmatrix}.$$
Per esempio, nella figura sotto ${\boldsymbol x}_1=\binom{3}{1}$ e
${\boldsymbol x}_2=\binom{1}{4}$, ed in rosso è rappresentata la loro
somma $2{\boldsymbol x}_1+{\boldsymbol x}_2=\binom{7}{6}$.&lt;/p&gt;
&lt;img src=&#34;https://gsicuro.github.io/images/im2/main-6.jpg&#34; alt=&#34;Somma di vettori nel piano&#34; style=&#34;width:50%;&#34;/&gt; 
&lt;/div&gt;
&lt;p&gt;Se ${\boldsymbol v}_1,\dots,{\boldsymbol v}_k\in\mathbb V$ e
$c_1,\dots,c_k\in{\mathbb K}$, il vettore
${\boldsymbol v}=c_1{\boldsymbol v}_1+\dots+c_k{\boldsymbol v}_k\in\mathbb V$
si dice essere una combinazione lineare di
${\boldsymbol v}_1,\dots,{\boldsymbol v}_k$ a coefficienti
$c_1,\dots,c_k$. Il concetto di combinazione lineare è cruciale e
caratterizza le proprietà di uno spazio vettoriale. Con un leggero
abuso, utilizzeremo la notazione&lt;/p&gt;
&lt;p&gt;$$c_1{\boldsymbol v}_1+\dots+c_k{\boldsymbol v}_k\equiv \sum_{i=1}^kc_i{\boldsymbol v}_i.$$&lt;/p&gt;
&lt;p&gt;Chiamiamo l&amp;rsquo;insieme di tutte le combinazioni lineari dei vettori
$\mathcal V\equiv \{{\boldsymbol v}_i\}_{i=1}^k$ lo &lt;em&gt;span&lt;/em&gt; di
$\{{\boldsymbol v}_i\}_{i=1}^k$ e scriviamo $\mathrm{span}(\mathcal V)$.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Definizione &lt;/b&gt;
I vettori $\{{\boldsymbol v}_i\}_{i=1}^k$ sono linearmente dipendenti se
esistono degli scalari $\{c_i\}_{i=1}^k$ non tutti nulli tali per cui
&lt;p&gt;$$\sum_{i=1}^kc_i{\boldsymbol v}_i=\mathbf 0;$$&lt;/p&gt;
&lt;p&gt;diversamente si dicono linearmente indipendenti.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Per definizione, il singolo vettore ${\boldsymbol v}$ è linearmente
dipendente se e solo se uguale al vettor nullo. Viceversa, se
${\boldsymbol v}_2=c{\boldsymbol v}_1$, ${\boldsymbol v}_1$ e
${\boldsymbol v}_2$ sono linearmente dipendenti. La definizione implica
che in un insieme di vettori linearmente dipendenti, almeno uno di essi
può sempre esprimersi come combinazione lineare degli altri. Infine,
vale la seguente proposizione.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Proposizione &lt;/b&gt;
Se $\{{\boldsymbol v}_i\}_{i=1}^k$ sono linearmente indipendenti,
allora, dati due set di scalari in ${\mathbb K}$ $\{a_i\}_{i=1}^k$ e
$\{b_i\}_{i=1}^k$
$$\sum_{i=1}^ka_i{\boldsymbol v}_i=\sum_{i=1}^kb_i{\boldsymbol v}_i\Rightarrow a_i=b_i\ \forall i=1,\dots,k.$$
&lt;/div&gt;
&lt;p&gt;Infine, possiamo introdurre il concetto di &lt;em&gt;base&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Definizione &lt;/b&gt;
Il set $\mathcal B\equiv \{{\boldsymbol v}_i\}_{i=1}^n$ di vettori di
$\mathbb V$ è una base se essi sono linearmente indipendenti e se
$\mathbb V=\mathrm{span}[\mathcal B]$.
&lt;/div&gt;
&lt;p&gt;Questo significa che ogni vettore ${\boldsymbol v}\in\mathbb V$ può
scriversi &lt;em&gt;in maniera unica&lt;/em&gt; come
${\boldsymbol v}=\sum_{i=1}^n c_i{\boldsymbol v}_i$, con coefficienti
$c_i$ che sono detti &lt;em&gt;coordinate&lt;/em&gt; di ${\boldsymbol v}$ secondo la base
$\mathcal B$. Ciò comporta anche che non è possibile avere un set di
vettori indipendenti con più di $n$ elementi. Vale infatti il seguente&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Teorema &lt;/b&gt;
Sia $\mathcal B\equiv  \{{\boldsymbol v}_i\}_{i=1}^n$ una base di
$\mathbb V$. Allora ogni set $\{{\boldsymbol u}_i\}_{i=1}^m$ di $m&gt;n$
vettori di $\mathbb V$ è costituito da elementi linearmente dipendenti.
Di conseguenza, ogni base di $\mathbb V$ ha $n$ elementi: il numero $n$
prende il nome di dimensione di $\mathbb V$.
&lt;/div&gt;
&lt;div class=&#34;alert alert-success&#34; role=&#34;alert&#34;&gt;
Lo spazio vettoriale ${\mathbb R}^n$ su ${\mathbb R}$ è costituito da
vettori $n$-dimensionali, rappresentabili come vettori colonna,
&lt;p&gt;$${\boldsymbol x}=\begin{pmatrix}x_1\\ \vdots\\x_n\end{pmatrix}.$$&lt;/p&gt;
&lt;p&gt;In questo caso è facilmente identificata una &lt;em&gt;base canonica&lt;/em&gt;
$\mathcal B=\{{\boldsymbol e}_i\}_{i=1}^n$ di $n$ vettori&lt;/p&gt;
&lt;p&gt;$${\boldsymbol e}_1\equiv
\begin{pmatrix}1\\0\\ \vdots\\0\end{pmatrix}
\quad{\boldsymbol e}_2\equiv
\begin{pmatrix}0\\1\\ \vdots\\0\end{pmatrix}
\quad\dots
\quad{\boldsymbol e}_n\equiv
\begin{pmatrix}0\\0\\ \vdots\\1\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;che permette di scrivere in maniera univoca qualunque vettore
${\boldsymbol x}\in{\mathbb R}^n$ come&lt;/p&gt;
&lt;p&gt;$${\boldsymbol x}=x_1
{\boldsymbol e}_1+\dots+
x_n{\boldsymbol e}_n.$$&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&#34;sottospazi-vettoriali&#34;&gt;Sottospazi vettoriali&lt;/h4&gt;
&lt;p&gt;Dato uno spazio vettoriale $\mathbb V$, è possibile identificare in
alcuni casi un &lt;em&gt;sottospazio&lt;/em&gt; vettoriale ${\mathbb W}$, ovvero un
sottoinsieme di $\mathbb V$ che è chiuso sotto le operazioni di somma e
prodotto per uno scalare, ovvero&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt; &lt;b&gt; Definizione &lt;/b&gt;
Dato uno ${\mathbb K}$-spazio vettoriale $\mathbb V$, un sottoinsieme
non vuoto $\mathbb W\subset\mathbb V$ è un sottospazio vettoriale di
$\mathbb V$ se, per ogni ${\boldsymbol w},{\boldsymbol w}&#39;\in\mathbb W$,
${\boldsymbol w}+{\boldsymbol w}&#39;\in\mathbb W$, e per ogni
$c\in{\mathbb K}$, se ${\boldsymbol w}\in\mathbb W$ allora
$c{\boldsymbol w}\in\mathbb W$.
&lt;/div&gt;
&lt;p&gt;Per esempio, se
$\mathcal V\subset\mathbb V$ è un sottoinsieme finito di $\mathbb V$,
$\mathrm{span}[\mathcal V]$ è un sottospazio di $\mathbb V$ (in
particolare, lo è anche per $\mathcal V$ ha cardinalità 1). Inoltre la dimensione di un
sottospazio è sempre limitata superiormente dalla dimensione dello
spazio in cui vive. È interessante notare che se $\mathbb U$ e
$\mathbb W$ sono sottospazi vettoriali di $\mathbb V$, allora
$\mathbb U\cap\mathbb W$ è anche sottospazio vettoriale di $\mathbb V$
(mentre la loro unione, in generale, non lo è).&lt;/p&gt;
&lt;div class=&#34;alert alert-success&#34; role=&#34;alert&#34;&gt;
Se $\mathbb V=\mathbb R^2$, per esempio, dato un vettore non nullo $\boldsymbol v\in\mathbb R^2$, un sottospazio vettoriale può costruirsi semplicemente considerando $\mathbb W=\{\boldsymbol w\in\mathbb V\colon \boldsymbol w=c\boldsymbol v,\ c\in\mathbb R\}$. Esso corrisponde ad una retta lungo la direzione di $\boldsymbol v$.
&lt;/div&gt;
&lt;p&gt;Dati due sottospazi vettoriali $\mathbb U$ e $\mathbb W$, se
$\mathbb U\cap\mathbb W={\mathbf 0}$ è possibile costruire un nuovo
spazio, detto &lt;em&gt;somma diretta di $\mathbb U$ e $\mathbb W$&lt;/em&gt; e indicato
con $\mathbb U\oplus\mathbb W$. Questo spazio consiste di tutti i
vettori ${\boldsymbol v}$ nella forma
${\boldsymbol v}={\boldsymbol u}+{\boldsymbol w}$ con
${\boldsymbol u}\in\mathbb U$, ${\boldsymbol w}\in\mathbb W$. Inoltre
questa decomposizione è unica: se infatti assumessimo che esiste
un&amp;rsquo;altra coppia tale per cui
${\boldsymbol v}={\boldsymbol u}&amp;rsquo;+{\boldsymbol w}&amp;rsquo;$, con
${\boldsymbol u}&amp;rsquo;\in\mathbb U$, ${\boldsymbol w}&amp;rsquo;\in\mathbb W$, allora
dovremmo avere
${\boldsymbol u}+{\boldsymbol w}={\boldsymbol u}&amp;rsquo;+{\boldsymbol w}&amp;rsquo;$ e
quindi
${\boldsymbol u}-{\boldsymbol u}&amp;rsquo;={\boldsymbol w}-{\boldsymbol w}&amp;rsquo;$. Ma
questa quantità può essere solo $\mathbf 0$, unico elemento
nell&amp;rsquo;intersezione dei due spazi.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matrici</title>
      <link>https://gsicuro.github.io/lectures/im2on/ch1/sec2/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/im2on/ch1/sec2/</guid>
      <description>&lt;h2 id=&#34;matrici&#34;&gt;Matrici&lt;/h2&gt;
&lt;p&gt;Un esempio particolarmente importante e prototipico di spazio vettoriale
è costituito dallo spazio delle matrici di elementi in ${\mathbb K}$.
Consideriamo ora due interi positivi $m$ e $n$. Una &lt;em&gt;matrice&lt;/em&gt; di
$m\times n$ elementi in ${\mathbb K}$ è una tabella del tipo&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}=
\begin{pmatrix}a_{11}&amp;amp;a_{12}&amp;amp;\dots&amp;amp;a_{1n}\\
a_{21}&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{2n}\\
\cdots&amp;amp;\cdots&amp;amp;\ddots&amp;amp;\vdots\\
a_{m1}&amp;amp;a_{m2}&amp;amp;\dots&amp;amp;a_{mn}
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;dove, $\forall \mu,\nu$, $a_{\mu\nu}\in{\mathbb K}$: il primo indice
denota la riga dell&amp;rsquo;elemento nella tabella, il secondo la sua colonna.
Scriveremo ${\boldsymbol A}=(a_{\mu\nu})_{\mu\nu}\in\mathcal M_{m,n}({\mathbb K})$.
A volte si denota&lt;/p&gt;
&lt;p&gt;$${\boldsymbol a}_\mu=(a_{\mu 1},\dots, a_{\mu n})$$&lt;/p&gt;
&lt;p&gt;la $\mu$-esima riga e&lt;/p&gt;
&lt;p&gt;$${\boldsymbol a}^\nu=\begin{pmatrix}a_{1\nu}\\ \dots\\a_{m\nu}
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;la $\nu$-esima colonna. Ogni matrice
${\boldsymbol A}$ può essere associata ad una matrice
${\boldsymbol A}^\intercal\in\mathcal M_{n,m}({\mathbb K})$ detta
&lt;em&gt;trasposta&lt;/em&gt; ottenuta invertendo righe con colonne:&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}=
\begin{pmatrix}a_{11}&amp;amp;a_{12}&amp;amp;\dots&amp;amp;a_{1n}\\
a_{21}&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{2n}\\
\cdots&amp;amp;\cdots&amp;amp;\ddots&amp;amp;\vdots\\
a_{m1}&amp;amp;a_{m2}&amp;amp;\dots&amp;amp;a_{mn}
\end{pmatrix}
\Rightarrow {\boldsymbol A}^\intercal=
\begin{pmatrix}a_{11}&amp;amp;a_{21}&amp;amp;\dots&amp;amp;a_{m1}\\
a_{12}&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{m2}\\
\cdots&amp;amp;\cdots&amp;amp;\ddots&amp;amp;\vdots\\
a_{1n}&amp;amp;a_{2n}&amp;amp;\dots&amp;amp;a_{mn}
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;operazione di trasposizione è quindi tale che
$\intercal\colon\mathcal M_{n,m}({\mathbb K})\to \mathcal M_{m,n}({\mathbb K})$.
Matrici con $n=1$ sono in particolare dette &lt;em&gt;vettori colonna&lt;/em&gt;, e viene
tipicamente usata la notazione
$\mathcal M_{m,1}({\mathbb K})\equiv {\mathbb K}^m$. Lo spazio delle
matrici su $\mathcal M_{m,n}({\mathbb K})$ costituisce uno spazio
vettoriale se introduciamo le seguenti operazioni. Date due matrici
${\boldsymbol A}=(a_{\mu\nu})&lt;em&gt;{\mu\nu},{\boldsymbol B}=(b&lt;/em&gt;{\mu\nu})&lt;em&gt;{\mu\nu}\in \mathcal M&lt;/em&gt;{m,n}({\mathbb K})$,
e uno scalare $c\in{\mathbb K}$, allora definiamo&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}+{\boldsymbol B}\equiv  (a_{\mu\nu}+b_{\mu\nu})&lt;em&gt;{\mu\nu}\in \mathcal M&lt;/em&gt;{m,n}({\mathbb K}),\qquad c{\boldsymbol A}\equiv  (ca_{\mu\nu})&lt;em&gt;{\mu\nu}\in\mathcal M&lt;/em&gt;{m,n}({\mathbb K}).$$&lt;/p&gt;
&lt;p&gt;Queste operazioni soddisfano le proprietà richieste per rendere
$\mathcal M_{m,n}({\mathbb K})$ uno spazio vettoriale secondo la
Definizione
&lt;a href=&#34;#def:spaziovettoriale&#34;&gt;[def:spaziovettoriale]&lt;/a&gt;{reference-type=&amp;ldquo;ref&amp;rdquo;
reference=&amp;ldquo;def:spaziovettoriale&amp;rdquo;}. Questo spazio ha dimensione $mn$. Una
base è costuita dalle matrici
${\boldsymbol E}&lt;em&gt;{ij}=(\delta&lt;/em&gt;{i\mu}\delta_{j\nu})_{\mu\nu}$.&lt;/p&gt;
&lt;h3 id=&#34;prodotto-tra-matrici&#34;&gt;Prodotto tra matrici&lt;/h3&gt;
&lt;p&gt;È possibile definire ora una nuova operazione, quella di &lt;em&gt;prodotto tra
matrici&lt;/em&gt;. Consideriamo una matrice
${\boldsymbol A}\in\mathcal M_{n,k}({\mathbb K})$ e una matrice
${\boldsymbol B}\in\mathcal M_{k,m}({\mathbb K})$. Allora è possibile
introdurre una matrice
${\boldsymbol C}=(c_{\mu\nu})&lt;em&gt;{\mu\nu}={\boldsymbol A}{\boldsymbol B}\in\mathcal M&lt;/em&gt;{n,m}({\mathbb K})$
tale che&lt;/p&gt;
&lt;p&gt;$$c_{\mu\nu}=\sum_{\rho=1}^ka_{\mu\rho}b_{\rho\nu}.$$&lt;/p&gt;
&lt;p&gt;In particolare, se ${\boldsymbol u}$ è un $n$-vettore riga&lt;/p&gt;
&lt;p&gt;$${\boldsymbol u}=
\begin{pmatrix}
u_1&amp;amp;\dots&amp;amp; u_n
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;e ${\boldsymbol v}$ un $n$-vettore colonna,&lt;/p&gt;
&lt;p&gt;$${\boldsymbol v}=
\begin{pmatrix}
v_1\\dots\ v_n
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;è possibile definire il &lt;em&gt;prodotto scalare&lt;/em&gt; tra i due come&lt;/p&gt;
&lt;p&gt;$${\boldsymbol u}{\boldsymbol v}=\sum_{\nu=1}^n u_\nu v_\nu.$$&lt;/p&gt;
&lt;p&gt;Nel caso dei vettori colonna, in particolare, si indica con
$|{\boldsymbol v}|^2\equiv {\boldsymbol v}^\intercal{\boldsymbol v}=\sum_{\nu=1}^nv_\nu^2$
la loro &lt;em&gt;norma&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Notare che l&amp;rsquo;operazione di prodotto tra matrici &lt;em&gt;non è abeliana&lt;/em&gt; e anzi
se $n\neq m$ il prodotto ${\boldsymbol B}{\boldsymbol A}$ è non
definito. Anche qualora l&amp;rsquo;operazione fosse possibile, per esempio se
$n=m$,
${\boldsymbol A}{\boldsymbol B}\neq {\boldsymbol B}{\boldsymbol A}$ in
generale.&lt;/p&gt;
&lt;p&gt;Le proprietà del prodotto tra matrici sono date dalla seguente
Proposizione, la cui dimostrazione avviene controllando esplicitamente
la validità di ogni affermazione.&lt;/p&gt;
&lt;p&gt;Siano ${\boldsymbol A},{\boldsymbol B}\in\mathcal M_{m,n}({\mathbb K})$
e ${\boldsymbol C},{\boldsymbol D}\in\mathcal M_{n,p}({\mathbb K})$, e
sia $c\in{\mathbb K}$. Allora&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$({\boldsymbol A}+{\boldsymbol B}){\boldsymbol C}={\boldsymbol A}{\boldsymbol C}+{\boldsymbol B}{\boldsymbol C}$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;${\boldsymbol A}({\boldsymbol C}+{\boldsymbol D})={\boldsymbol A}{\boldsymbol C}+{\boldsymbol A}{\boldsymbol D}$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;${\boldsymbol A}(c{\boldsymbol C})=c({\boldsymbol A}{\boldsymbol C})=(c{\boldsymbol A}){\boldsymbol C}$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;${\boldsymbol A}{\boldsymbol I}_n={\boldsymbol A}={\boldsymbol I}_m{\boldsymbol A}$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$({\boldsymbol A}{\boldsymbol C})^\intercal={\boldsymbol C}^\intercal{\boldsymbol A}^\intercal$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$({\boldsymbol A}+{\boldsymbol B})^\intercal={\boldsymbol A}^\intercal+{\boldsymbol B}^\intercal$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$({\boldsymbol A}{\boldsymbol C}){\boldsymbol Q}={\boldsymbol A}({\boldsymbol C}{\boldsymbol Q})$
se ${\boldsymbol Q}\in{\mathbb K}^{p\times q}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrici-quadrate&#34;&gt;Matrici quadrate&lt;/h3&gt;
&lt;p&gt;Data una matrice ${\boldsymbol A}\in\mathcal M_{m,n}({\mathbb K})$, se
$m=n$ la matrice ${\boldsymbol A}$ è detta &lt;em&gt;quadrata&lt;/em&gt; di dimensione $n$.
Una matrice quadrata è diagonale se ha la forma
${\boldsymbol A}=(a_{\mu}\delta_{\mu\nu})&lt;em&gt;{\mu\nu}$, dove
$\delta&lt;/em&gt;{\mu\nu}$ è il simbolo di Kronecker. Tra le matrici diagonali la
&lt;em&gt;matrice unità&lt;/em&gt; ${\boldsymbol I}&lt;em&gt;n\equiv (\delta&lt;/em&gt;{\mu\nu})_{\mu\nu}$
ha un ruolo speciale dato che si comporta come l&amp;rsquo;identità rispetto al
prodotto tra matrici, come vedremo. Infine, una matrice quadrata
${\boldsymbol A}\in{\mathbb K}$ è &lt;em&gt;simmetrica&lt;/em&gt; se
${\boldsymbol A}={\boldsymbol A}^\intercal$ e &lt;em&gt;antisimmetrica&lt;/em&gt; se
${\boldsymbol A}=-{\boldsymbol A}^\intercal$. Elenchiamo ora alcune
proprietà delle matrici quadrate.&lt;/p&gt;
&lt;p&gt;Una matrice quadrata ${\boldsymbol A}\in\mathcal M_{n,n}({\mathbb K})$
si dice invertibile se esiste una matrice quadrata $n\times n$, che
denotiamo con ${\boldsymbol A}^{-1}$, tale che
${\boldsymbol A}{\boldsymbol A}^{-1}={\boldsymbol A}^{-1}{\boldsymbol A}={\boldsymbol I}_n$.&lt;/p&gt;
&lt;p&gt;Sia ${\boldsymbol A}\in\mathcal M_{n,n}({\mathbb K})$. La sua matrice
inversa ${\boldsymbol A}^{-1}$ se esiste è unica.&lt;/p&gt;
&lt;p&gt;Supponiamo che la tesi non sia vera e procediamo per assurdo, ovvero
ammettiamo che esista una matrice
$\hat{\boldsymbol A}^{-1}\neq{\boldsymbol A}^{-1}$ tale che
$\hat{\boldsymbol A}^{-1}{\boldsymbol A}={\boldsymbol I}_n$. Allora
${\boldsymbol A}^{-1}={\boldsymbol A}^{-1}{\boldsymbol A}\hat{\boldsymbol A}^{-1}=\hat{\boldsymbol A}^{-1}$
che è l&amp;rsquo;assurdo cercato.&lt;/p&gt;
&lt;p&gt;Dalla definizione, $({\boldsymbol A}^{-1})^{-1}={\boldsymbol A}$ e
inoltre ${\boldsymbol I}^{-1}_n={\boldsymbol I}_n$. Infine, vale la
seguente&lt;/p&gt;
&lt;p&gt;Siano ${\boldsymbol A}\in\mathcal M_{n,p}({\mathbb K})$ e
${\boldsymbol B}\in{\mathbb K}^{p\times m}$ entrambe dotate di inversa.
Allora
$({\boldsymbol A}{\boldsymbol B})^{-1}={\boldsymbol B}^{-1}{\boldsymbol A}^{-1}\in\mathcal M_{m,n}({\mathbb K})$.&lt;/p&gt;
&lt;p&gt;Il sottoinsieme di $\mathcal M_{n,n}({\mathbb K})$ dato dalle matrici
invertibili si denota $\mathsf{GL}_{n}({\mathbb K})$ ed ha la struttura
di &lt;em&gt;gruppo&lt;/em&gt;. Se ${\mathbb K}={\mathbb R}$, all&amp;rsquo;interno di tale
sottinsieme si trova il sottinsieme $\mathsf O_n({\mathbb R})$ delle
matrici &lt;em&gt;ortogonali&lt;/em&gt;, ovvero delle matrici ${\boldsymbol A}$ tali per
cui ${\boldsymbol A}^{-1}={\boldsymbol A}^\intercal$.&lt;/p&gt;
&lt;p&gt;Concludiamo questa sezione sulle matrici quadrate introducendo un&amp;rsquo;ultima
importante quantità, ovvero il &lt;em&gt;determinante&lt;/em&gt; di una matrice quadrata.&lt;/p&gt;
&lt;p&gt;Data una matrice quadrata
${\boldsymbol A}\in\mathcal M_{n,n}({\mathbb K})$, il determinante di
${\boldsymbol A}$ è uno scalare in ${\mathbb K}$ definito ricorsivamente
come&lt;/p&gt;
&lt;p&gt;$$\det{\boldsymbol A}=\sum_{\mu\nu}(-1)^{\mu+\nu} a_{\mu\nu}\det({\boldsymbol A}_{\mu\nu})$$&lt;/p&gt;
&lt;p&gt;dove ${\boldsymbol A}_{\mu\nu}\in{\mathbb K}^{(n-1)\times(n-1)}$ è la
matrice $(n-1)\times (n-1)$ ottenuta da ${\boldsymbol A}$ rimuovendo la
$\mu$ riga e la $\nu$ colonna. Inoltre, dato uno scalare
$c\in{\mathbb K}$, $\det(c)=c$.&lt;/p&gt;
&lt;p&gt;La definizione sopra, dovuta a Laplace, è ricorsiva ma permette di
ottenere l&amp;rsquo;espressione del determinante in tutti i casi. Per esempio,
per $n=2$&lt;/p&gt;
&lt;p&gt;$$\det
\begin{pmatrix}
a_{11}&amp;amp;a_{12}\
a_{21}&amp;amp;a_{22}
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=a_{11}a_{22}-a_{12}a_{21}.$$&lt;/p&gt;
&lt;p&gt;Nonostante la sua definizione apparentemente esotica, il determinante è,
come espresso dal nome, cruciale per comprendere molte proprietà delle
matrici quadrate. Esso stesso gode di alcune proprietà riassunte nel
seguente&lt;/p&gt;
&lt;p&gt;[[t:LinAlg:det]]{#t:LinAlg:det label=&amp;ldquo;t:LinAlg:det&amp;rdquo;} Sia
${\boldsymbol A}\in\mathcal M_{n\times n}({\mathbb K})$. Allora&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;se ${\boldsymbol A}$ è una matrice diagonale, allora
$\det{\boldsymbol A}=\prod_{i=1}^n a_{ii}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\det{\boldsymbol A}=\det{\boldsymbol A}^\intercal$ (e di
conseguenza proprietà riferite alle colonne si applicano anche alle
righe).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dato uno scalare $c\in{\mathbb K}$,
$\det (c{\boldsymbol A}) = c^n \det{\boldsymbol A}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sia una colonna di ${\boldsymbol A}$ tale che
${\boldsymbol a}^\nu=\lambda{\boldsymbol v}+{\boldsymbol u}$, i.e.,
$a_{\mu\nu}=\lambda v_\nu+u_\nu$. Allora&lt;/p&gt;
&lt;p&gt;$$\begin{gathered}
\det{\boldsymbol A}=\det
\left(
\begin{array}{cc&amp;gt;{\columncolor{gray!40}}ccc}
a_{11} &amp;amp; \dots &amp;amp; a_{1\nu} &amp;amp; \dots &amp;amp; a_{1n} \
\vdots &amp;amp;\vdots &amp;amp;\vdots&amp;amp;\vdots &amp;amp; \vdots\
a_{n1}&amp;amp;\cdots &amp;amp;a_{n\nu} &amp;amp;\cdots &amp;amp;a_{nn}
\end{array}&lt;/p&gt;
&lt;p&gt;\right)\=\lambda\det
\left(
\begin{array}{cc&amp;gt;{\columncolor{gray!40}}ccc}
a_{11} &amp;amp; \dots &amp;amp; v_{1} &amp;amp; \cdots &amp;amp; a_{1n} \
\vdots &amp;amp;\vdots &amp;amp;\vdots&amp;amp;\vdots &amp;amp; \vdots\
a_{n1}&amp;amp;\cdots &amp;amp;v_{n} &amp;amp;\cdots &amp;amp;a_{nn}
\end{array}&lt;/p&gt;
&lt;p&gt;\right)+\det
\left(
\begin{array}{cc&amp;gt;{\columncolor{gray!40}}ccc}
a_{11} &amp;amp; \dots &amp;amp; u_{1} &amp;amp; \dots &amp;amp; a_{1n} \
\vdots &amp;amp;\vdots &amp;amp;\vdots&amp;amp;\vdots &amp;amp; \vdots\
a_{n1}&amp;amp;\cdots &amp;amp;u_{n} &amp;amp;\cdots &amp;amp;a_{nn}
\end{array}&lt;/p&gt;
&lt;p&gt;\right)\end{gathered}
$$&lt;/p&gt;
&lt;p&gt;Si dice che di conseguenza il determinante è &lt;em&gt;multilineare&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Se ${\boldsymbol A}$ ha due colonne identiche,
$\det{\boldsymbol A}=0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sia ${\boldsymbol B}\in\mathcal M_{n\times n}({\mathbb K})$; allora
$\det {\boldsymbol A}{\boldsymbol B}=\det{\boldsymbol A}\det {\boldsymbol B}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Una delle conseguenze delle proprietà suddette è che il determinante è
&lt;em&gt;alternante&lt;/em&gt;, ovvero guadagna un segno se due colonne vengono scambiate:&lt;/p&gt;
&lt;p&gt;$$\begin{gathered}
\det
\left(
\begin{array}{cc&amp;gt;{\columncolor{gray!20}}cc&amp;gt;{\columncolor{gray!60}}ccc}
a_{11} &amp;amp; \dots &amp;amp; a_{1k} &amp;amp;\dots &amp;amp; a_{1k&amp;rsquo;} &amp;amp; \dots &amp;amp; a_{1n} \
\vdots &amp;amp;\vdots &amp;amp;\vdots&amp;amp;\vdots &amp;amp; \vdots&amp;amp;\vdots &amp;amp; \vdots\
a_{n1}&amp;amp;\cdots &amp;amp;a_{nk} &amp;amp;\cdots &amp;amp;a_{nk&amp;rsquo;}&amp;amp;\cdots &amp;amp; a_{nn}
\end{array}&lt;/p&gt;
&lt;p&gt;\right)\=-\det
\left(
\begin{array}{cc&amp;gt;{\columncolor{gray!60}}cc&amp;gt;{\columncolor{gray!20}}ccc}
a_{11} &amp;amp; \dots &amp;amp; a_{1k&amp;rsquo;} &amp;amp;\dots &amp;amp; a_{1k} &amp;amp; \dots &amp;amp; a_{1n} \
\vdots &amp;amp;\vdots &amp;amp;\vdots&amp;amp;\vdots &amp;amp; \vdots&amp;amp;\vdots &amp;amp; \vdots\
a_{n1}&amp;amp;\cdots &amp;amp;a_{nk&amp;rsquo;} &amp;amp;\cdots &amp;amp;a_{nk}&amp;amp;\cdots &amp;amp; a_{nn}
\end{array}&lt;/p&gt;
&lt;p&gt;\right)\end{gathered}
$$&lt;/p&gt;
&lt;p&gt;Il ruolo cruciale del determinante è legato anche al seguente lemma che
caratterizza l&amp;rsquo;invertibilità delle matrici quadrate.&lt;/p&gt;
&lt;p&gt;[[l:LinAlg1:inv]]{#l:LinAlg1:inv label=&amp;ldquo;l:LinAlg1:inv&amp;rdquo;} Sia
${\boldsymbol A}\in\mathcal M_{n\times n}({\mathbb K})$; sia
${\boldsymbol C}$ la matrice dei suoi &lt;em&gt;cofattori&lt;/em&gt;, ovvero la matrice
$n\times n$ con elementi&lt;/p&gt;
&lt;p&gt;$$c_{\mu\nu} = (-1)^{\mu\nu}\det{\boldsymbol A}_{\mu\nu}.$$ Allora, se
$\det{\boldsymbol A}\neq 0$, l&amp;rsquo;inversa di ${\boldsymbol A}$ esiste ed è
data da&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}^{-1} = \frac{1}{\det{\boldsymbol A}} {\boldsymbol C}^\intercal.$$&lt;/p&gt;
&lt;h2 id=&#34;sistemi-di-equazioni-lineari&#34;&gt;Sistemi di equazioni lineari&lt;/h2&gt;
&lt;p&gt;Le definizioni date finora sono funzionali ad una serie di importanti
applicazioni. La prima che andremo a considerare è la soluzione di
&lt;em&gt;sistemi lineari di equazioni&lt;/em&gt;, ovvero sistemi nella forma&lt;/p&gt;
&lt;p&gt;$$\label{eq:linsyst}&lt;/p&gt;
&lt;p&gt;\begin{cases}
a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n=b_1\
a_{21}x_1+a_{22}x_2+\dots+a_{2n}x_n=b_2\
\vdots\
a_{m1}x_1+a_{m2}x_2+\dots+a_{mn}x_n=b_m
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;in cui si assume che i coefficienti $a_{\mu\nu}\in{\mathbb K}$ e
$b_\nu\in{\mathbb K}$ siano noti e l&amp;rsquo;obiettivo è ottenere
$x_\nu\in{\mathbb K}$. Indicando con
${\boldsymbol A}=(a_{\mu\nu})&lt;em&gt;{\mu\nu}\in\mathcal M&lt;/em&gt;{m,n}({\mathbb K})$,
${\boldsymbol b}=(b_\nu)&lt;em&gt;\nu\in{\mathbb K}^m$,
${\boldsymbol x}=(x&lt;/em&gt;\nu)_{\nu}\in{\mathbb K}^n$, possiamo scrivere il
sistema sopra in una forma molto più compatta come&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}.$$&lt;/p&gt;
&lt;p&gt;Il sistema di equazioni è detto &lt;em&gt;omogeneo&lt;/em&gt; se
${\boldsymbol b}=\mathbf 0$, mentre diversamente è detto &lt;em&gt;non omogeneo&lt;/em&gt;.
Ogni sistema è detto &lt;em&gt;compatibile&lt;/em&gt; se ha almeno una soluzione: notare
che un sistema omogeneo ha sempre la soluzione &lt;em&gt;banale&lt;/em&gt;
${\boldsymbol x}=\mathbf 0$. Se è dato un sistema non omogeneo
${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}$,
${\boldsymbol b}\neq\mathbf 0$, allora il sistema omogeneo ad esso
associato è ${\boldsymbol A}{\boldsymbol x}=\mathbf 0$.&lt;/p&gt;
&lt;h3 id=&#34;metodo-di-eliminazione-di-gauss&#34;&gt;Metodo di eliminazione di Gauss&lt;/h3&gt;
&lt;p&gt;Risolvere un sistema tipo
${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}$ è il problema centrale
dell&amp;rsquo;algebra lineare. L&amp;rsquo;idea generale è utilizzare operazioni matriciali
per poter infine ottenere una espressione per la soluzione
${\boldsymbol x}$, se essa esiste. Un primo approccio che possiamo
tentare è il cosiddetto metodo di eliminazione di Gauss. Supponiamo di
avere un sistema come in
Eq. &lt;a href=&#34;#eq:linsyst&#34;&gt;[eq:linsyst]&lt;/a&gt;{reference-type=&amp;ldquo;eqref&amp;rdquo;
reference=&amp;ldquo;eq:linsyst&amp;rdquo;}. Questo può essere associato alla seguente
matrice&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
a_{11}&amp;amp;a_{12}&amp;amp;\dots&amp;amp;a_{1n}&amp;amp;b_1\\
a_{21}&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{2n}&amp;amp;b_2\\
\cdots&amp;amp;\cdots&amp;amp;\ddots&amp;amp;\vdots\\
a_{m1}&amp;amp;a_{m2}&amp;amp;\dots&amp;amp;a_{mn}&amp;amp;b_m
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;ottenuta concatenando la matrice ${\boldsymbol A}$ e la colonna
${\boldsymbol b}$ e talvolta detta &lt;em&gt;matrice orlata&lt;/em&gt;. Ogni matrice
siffatta corrisponde ad un sistema come in
Eq. &lt;a href=&#34;#eq:linsyst&#34;&gt;[eq:linsyst]&lt;/a&gt;{reference-type=&amp;ldquo;eqref&amp;rdquo;
reference=&amp;ldquo;eq:linsyst&amp;rdquo;}. Possiamo eseguire una serie di operazioni di
riga su questa matrice che lasciano inalterato il problema. In
particolare&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;possiamo moltiplicare una riga per una costante $c\in{\mathbb K}$
che sia non nulla;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;possiamo scambiare due righe;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;possiamo aggiungere ad una riga un multiplo di un&amp;rsquo;altra riga.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questo tipo di operazioni può essere utilizzato per risolvere il sistema
di equazioni lineari. Vediamo come con un esempio.&lt;/p&gt;
&lt;p&gt;Cerchiamo di risolvere il sistema&lt;/p&gt;
&lt;p&gt;$$\begin{cases}
2x_1+4x_2-2x_3=2\
4x_1+9x_2-3x_3=8\
-2x_1-3x_2+7x_3=10
\end{cases}&lt;/p&gt;
&lt;p&gt;\Leftrightarrow&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
2&amp;amp;4&amp;amp;-2\
4&amp;amp;9&amp;amp;-3\
-2&amp;amp;-3&amp;amp;7
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
x_1\x_2\x_3
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=
\begin{pmatrix}2\8\10
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;così che la matrice orlata è&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
4&amp;amp;9&amp;amp;-3&amp;amp;8\
-2&amp;amp;-3&amp;amp;7&amp;amp;10
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Proviamo ora a eseguire una operazione tale per cui la variabile $x_1$
venga rimossa da tutte le equazioni eccetto la prima. Ciò equivale a
dire che gli elementi sotto $a_{11}$ devono essere trasformati in zero
eseguendo una delle operazioni di riga indicate sopra. Per esempio,
sostituiamo la $i$-esima riga con
$(\text{$i$ riga})-\frac{a_{i1}}{a_{11}}(\text{prima riga})$. L&amp;rsquo;elemento
$a_{11}$ è detto &lt;em&gt;primo pivot&lt;/em&gt; ed è cruciale che non sia zero. Se
dovesse esserlo, basta scambiare le righe in maniera tale che compaia
come prima una riga con $a_{11}\neq 0$. Si ottiene quindi&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
0&amp;amp;1&amp;amp;1&amp;amp;4\
0&amp;amp;1&amp;amp;5&amp;amp;12
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Ripetiamo la procedura partendo dalla &lt;em&gt;seconda&lt;/em&gt; riga verso il basso.
Questa volta il pivot è $a_{22}=1$ e si procede come sopra per le righe
$i&amp;gt;2$: $(\text{$i$ riga})-\frac{a_{i2}}{a_{22}}(\text{prima riga})$,
ottenendo&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
0&amp;amp;1&amp;amp;1&amp;amp;4\
0&amp;amp;0&amp;amp;4&amp;amp;8
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Questa matrice è a gradini e siamo arrivati alla forma desiderata.
Perché questo è utile? Se scriviamo il sistema associato, esso è&lt;/p&gt;
&lt;p&gt;$$\begin{cases}
2x_1+4x_2-2x_3=2\
x_2+x_3=4\
4x_3=8
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;che può essere risolto all&amp;rsquo;indietro, ovvero risolvendo prima per $x_3$,
ottenendo $x_3=2$, la cui soluzione può essere sostituita nella seconda
riga che dà $x_2=2$ e infine inserendo entrambe queste soluzioni nella
prima, che fornisce $x_1=-1$.&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;idea del &lt;em&gt;metodo di eliminazione di Gauss&lt;/em&gt; consiste quindi nel
trasformare la matrice orlata in una &lt;em&gt;matrice a gradini&lt;/em&gt; che ammetta più
facilmente una soluzione. Se $m\leq n$, tale matrice finale avrà la
forma&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
a_{11}&amp;amp;a_{12}&amp;amp;\dots&amp;amp;a_{1m}&amp;amp;\dots&amp;amp;a_{1n}&amp;amp;b_1\
0&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{2m}&amp;amp;\dots&amp;amp;a_{2n}&amp;amp;b_2\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\cdots&amp;amp;\cdots&amp;amp;\vdots&amp;amp;\vdots\
0&amp;amp;0&amp;amp;\dots&amp;amp;a_{mm}&amp;amp;\dots&amp;amp;a_{mn}&amp;amp;b_m
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$&lt;/p&gt;
&lt;p&gt;Una matrice a gradini è tale per cui, per ogni riga, il primo elemento
non-nullo si trova a destra del primo elemento non-nullo della riga
precedente. Supponiamo ora che $\prod_{i=1}^ma_{ii}\neq 0$. L&amp;rsquo;esempio
mostra che, se $m=n$ e il nostro algoritmo ci fornisce la matrice orlata
a gradini, allora il sistema è risolubile: l&amp;rsquo;ultima riga fornisce
l&amp;rsquo;equazione per $x_n$, la penultima per $x_{n-1}$ e così via: in questo
caso la soluzione del sistema è &lt;em&gt;unica&lt;/em&gt;. Se invece $m&amp;lt;n$, l&amp;rsquo;ultima
equazione è semplicemente&lt;/p&gt;
&lt;p&gt;$$a_{mm}x_m+\dots+a_{mn}x_n=b_m$$ ovvero ci dice che la variabile $x_m$
può essere espressa in termini delle variabili $x_{m+1},\dots,x_{n}$
come $x_m=\frac{1}{a_{mm}}\left(b_m-\sum_{i=m+1}^na_{mi}x_i\right)$. Non
abbiamo sufficiente informazione per fissare le variabili $x_i$ con
$m+1\leq i\leq n$ che peranto rimangono &lt;em&gt;parametri arbitrari&lt;/em&gt; fissati i
quali tutte le altre variabili possono essere fissate univocamente.
Questo vuol dire che le soluzioni del nostro sistema vivono in uno
spazio $n-m$ dimensionale.&lt;/p&gt;
&lt;p&gt;Tuttavia il metodo &lt;em&gt;può fallire&lt;/em&gt; se la matrice a gradini ottenuta ha
$\prod_{i=1}^ma_{ii}=0$ o se $m&amp;gt;n$.&lt;/p&gt;
&lt;p&gt;Cerchiamo di risolvere il sistema&lt;/p&gt;
&lt;p&gt;$$\begin{cases}
2x_1+4x_2-2x_3=2\
4x_1+8x_2-3x_3=8\
-2x_1-4x_2+7x_3=10
\end{cases}&lt;/p&gt;
&lt;p&gt;\Leftrightarrow&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
2&amp;amp;4&amp;amp;-2\
4&amp;amp;8&amp;amp;-3\
-2&amp;amp;-4&amp;amp;7
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
x_1\x_2\x_3
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=
\begin{pmatrix}2\8\10
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;così che la matrice orlata è&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
4&amp;amp;8&amp;amp;-3&amp;amp;8\
-2&amp;amp;-4&amp;amp;7&amp;amp;10
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Seguiamo la procedura di Gauss. Al primo step otteniamo&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
0&amp;amp;0&amp;amp;1&amp;amp;4\
0&amp;amp;0&amp;amp;5&amp;amp;12
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;mentre al secondo&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;4&amp;amp;-2&amp;amp;2\
0&amp;amp;0&amp;amp;1&amp;amp;4\
0&amp;amp;0&amp;amp;0&amp;amp;-8
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;ultima riga in particolare corrisponde ad una equazione &lt;em&gt;falsa&lt;/em&gt;,
ovvero $0=-8$. Questo significa che il sistema è &lt;em&gt;incompatibile&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Se $m&amp;gt;n$ la matrice orlata a gradini finale avrà una forma tipo&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
a_{11}&amp;amp;a_{12}&amp;amp;\dots&amp;amp;a_{1m}&amp;amp;\dots&amp;amp;a_{1n}&amp;amp;b_1\
0&amp;amp;a_{22}&amp;amp;\dots&amp;amp;a_{2m}&amp;amp;\dots&amp;amp;a_{2n}&amp;amp;b_2\
\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\cdots&amp;amp;\cdots&amp;amp;\vdots&amp;amp;\vdots\
0&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;\dots&amp;amp;a_{n,n}&amp;amp;b_{n}\
0&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;b_{n+1}\
0&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;\vdots\
0&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;\dots&amp;amp;0&amp;amp;b_{m}\
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$&lt;/p&gt;
&lt;p&gt;Se, per via della procedura, $b_{n+1}=\dots=b_m=0$, allora possiamo
trascurare tutte le righe dalla $(n+1)$esima in poi, dato che non sono
informative, e ci riduciamo ad un caso in cui di fatto $m=n$.
Diversamente, se uno dei valori $b_i\neq0$ per $n+1\leq i\leq m$, allora
la procedura è risultata in una condizione falsa, che vuol dire che il
nostro sistema è incompatibile e non esistono soluzioni.&lt;/p&gt;
&lt;p&gt;In conclusione, per $m\leq n$, &lt;em&gt;il sistema è compatibile se la matrice
orlata può essere messa in una forma a gradini con
$\prod_{i=1}^ma_{ii}\neq 0$.&lt;/em&gt; Se $n=m$ questa condizione è sufficiente a
garantire che la soluzione è unica. In tutti gli altri casi, il sistema
è incompatibile se la procedura genera equazioni nella forma $0=b_i$ con
$b_i\neq 0$.&lt;/p&gt;
&lt;h3 id=&#34;metodo-di-gauss--jordan-per-linversa&#34;&gt;Metodo di Gauss&amp;ndash;Jordan per l&amp;rsquo;inversa&lt;/h3&gt;
&lt;p&gt;Intuitivamente, il metodo di Gauss descritto sopra, quando ha successo,
permette di &amp;ldquo;invertire&amp;rdquo; la matrice ${\boldsymbol A}$ nell&amp;rsquo;equazione
${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}$ e, in particolare, ci
aspettiamo che, se ${\boldsymbol A}$ è una matrice quadrata invertibile,
allora in effetti ${\boldsymbol x}={\boldsymbol A}^{-1}{\boldsymbol b}$.
In qualche modo, in questo caso, il metodo produce esattamente il
risultato di questa operazione applicando l&amp;rsquo;inversa di ${\boldsymbol A}$
a ${\boldsymbol b}$, quando questa esiste.&lt;/p&gt;
&lt;p&gt;In effetti, supponiamo che la matrice
${\boldsymbol A}\in\mathcal M_{n,n}({\mathbb K})$. La matrice inversa
${\boldsymbol A}^{-1}$ è tale per cui
${\boldsymbol A}{\boldsymbol A}^{-1}={\boldsymbol I}_n$. Se denoto con
${\boldsymbol x}_i$ la $i$-esima colonna di ${\boldsymbol A}^{-1}$&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}{\boldsymbol x}_i={\boldsymbol e}&lt;em&gt;i,\qquad \text{dove}\quad {\boldsymbol e}&lt;em&gt;i=(\delta&lt;/em&gt;{\nu i})&lt;/em&gt;{\nu}\quad \text{per }i=1,\dots, n.$$&lt;/p&gt;
&lt;p&gt;In sostanza trovare l&amp;rsquo;inversa equivale a risolvere contemporaneamente
$n$ sistemi di equazioni lineari, problema per il quale possiamo
applicare il metodo di Gauss. Per meglio esemplificare questo fatto,
ricorriamo ad un esempio.&lt;/p&gt;
&lt;p&gt;Consideriamo la matrice quadrata&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}=
\begin{pmatrix}
2&amp;amp;-1&amp;amp;0\
-1&amp;amp;2&amp;amp;-1\
0&amp;amp;-1&amp;amp;2\
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$ Per trovare l&amp;rsquo;inversa&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}^{-1}=
\begin{pmatrix}
x_{11}&amp;amp;x_{12}&amp;amp;x_{13}\
x_{21}&amp;amp;x_{22}&amp;amp;x_{23}\
x_{31}&amp;amp;x_{22}&amp;amp;x_{33}\
\end{pmatrix}
$$ occorre risolvere le equazioni&lt;/p&gt;
&lt;p&gt;$$\medmuskip=0mu
\thinmuskip=0mu
\thickmuskip=0mu
\begin{pmatrix}
2&amp;amp;-1&amp;amp;0\
-1&amp;amp;2&amp;amp;-1\
0&amp;amp;-1&amp;amp;2\
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
x_{11}\x_{21}\x_{31}
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=
\begin{pmatrix}
1\0\0
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;,\qquad
\begin{pmatrix}
2&amp;amp;-1&amp;amp;0\
-1&amp;amp;2&amp;amp;-1\
0&amp;amp;-1&amp;amp;2\
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
x_{11}\x_{21}\x_{31}
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=
\begin{pmatrix}
0\1\0
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;,\qquad
\begin{pmatrix}
2&amp;amp;-1&amp;amp;0\
-1&amp;amp;2&amp;amp;-1\
0&amp;amp;-1&amp;amp;2\
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\begin{pmatrix}
x_{11}\x_{21}\x_{31}
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;=
\begin{pmatrix}
0\0\1
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$ Possiamo immaginare di procedere parallelamente su tutti i sistemi
usando una matrice orlata&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;-1&amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\
-1&amp;amp;2&amp;amp;-1&amp;amp;0&amp;amp;1&amp;amp;0\
0&amp;amp;-1&amp;amp;2&amp;amp;0&amp;amp;0&amp;amp;1\
\end{pmatrix}
$$ Questa matrice orlata ha la forma&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
{\boldsymbol A}&amp;amp;{\boldsymbol I}
\end{pmatrix}
$$ Usando la procedura di Gauss per mettere la matrice in
forma a gradini si ottiene&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;-1&amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\
0&amp;amp;\sfrac{3}{2}&amp;amp;-1&amp;amp;\sfrac{1}{2}&amp;amp;1&amp;amp;0\
0&amp;amp;0&amp;amp;\sfrac{4}{3}&amp;amp;\sfrac{2}{3}&amp;amp;\sfrac{2}{3}&amp;amp;1\
\end{pmatrix}
$$ A questo punto vorremmo mettere il sistema nella forma&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
{\boldsymbol I}&amp;amp;{\boldsymbol B}
\end{pmatrix}
$$ perché in questo caso
${\boldsymbol B}={\boldsymbol A}^{-1}$! La matrice orlata ottenuta
infatti corrisponderebbe al set di sistemi $x_{\mu\nu}=b_{\mu\nu}$
fornendoci la soluzione che cerchiamo. Dobbiamo quindi lavorare per
rimuovere gli zeri &lt;em&gt;sopra&lt;/em&gt; la diagonale procedendo stavolta dal basso
verso l&amp;rsquo;alto.&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
2&amp;amp;-1&amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\
0&amp;amp;\sfrac{3}{2}&amp;amp;-1&amp;amp;\sfrac{3}{4}&amp;amp;\sfrac{3}{2}&amp;amp;\sfrac{3}{4}\
0&amp;amp;0&amp;amp;\sfrac{4}{3}&amp;amp;\sfrac{2}{3}&amp;amp;\sfrac{2}{3}&amp;amp;1
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\Rightarrow
\begin{pmatrix}
2&amp;amp;-1&amp;amp;0&amp;amp;1&amp;amp;0&amp;amp;0\
0&amp;amp;\sfrac{3}{2}&amp;amp;0&amp;amp;\sfrac{3}{4}&amp;amp;\sfrac{3}{2}&amp;amp;\sfrac{3}{4}\
0&amp;amp;0&amp;amp;\sfrac{4}{3}&amp;amp;\sfrac{2}{3}&amp;amp;\sfrac{2}{3}&amp;amp;1
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\Rightarrow
\begin{pmatrix}
2&amp;amp;0&amp;amp;0&amp;amp;\sfrac{3}{2}&amp;amp;1&amp;amp;\sfrac{1}{2}\
0&amp;amp;\sfrac{3}{2}&amp;amp;0&amp;amp;\sfrac{3}{4}&amp;amp;\sfrac{3}{2}&amp;amp;\sfrac{3}{4}\
0&amp;amp;0&amp;amp;\sfrac{4}{3}&amp;amp;\sfrac{1}{3}&amp;amp;\sfrac{2}{3}&amp;amp;1
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$ Dividendo opportunamente ogni riga otteniamo&lt;/p&gt;
&lt;p&gt;$$\begin{pmatrix}
1&amp;amp;0&amp;amp;0&amp;amp;\sfrac{3}{4}&amp;amp;\sfrac{1}{2}&amp;amp;\sfrac{1}{4}\
0&amp;amp;1&amp;amp;0&amp;amp;\sfrac{1}{2}&amp;amp;1&amp;amp;\sfrac{1}{2}\
0&amp;amp;0&amp;amp;1&amp;amp;\sfrac{1}{4}&amp;amp;\sfrac{1}{2}&amp;amp;\sfrac{3}{4}
\end{pmatrix}
$$ per cui&lt;/p&gt;
&lt;p&gt;$${\boldsymbol A}^{-1}=
\begin{pmatrix}
\sfrac{3}{4}&amp;amp;\sfrac{1}{2}&amp;amp;\sfrac{1}{4}\
\sfrac{1}{2}&amp;amp;1&amp;amp;\sfrac{1}{2}\
\sfrac{1}{4}&amp;amp;\sfrac{1}{2}&amp;amp;\sfrac{3}{4}
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;.$$&lt;/p&gt;
&lt;p&gt;Il metodo descritto è detto &lt;em&gt;di Gauss&amp;ndash;Jordan&lt;/em&gt;. La procedura fallisce se
l&amp;rsquo;inversa cercata non esiste. Questo è il caso, per esempio, se &lt;em&gt;dopo&lt;/em&gt;
aver ottenuto la forma a gradini $\prod_{i=1}^na_{ii}=0$. Questa
condizione ha un significato preciso che specificheremo.&lt;/p&gt;
&lt;h3 id=&#34;rango&#34;&gt;Rango&lt;/h3&gt;
&lt;p&gt;Il metodo di eliminazione di Gauss permette di risolvere un sistema di
equazioni lineari, o determinarne l&amp;rsquo;incompatibilità. Non fornisce però
(almeno direttamente) un &lt;em&gt;criterio&lt;/em&gt; di risolubilità né è chiaro perché
in alcuni casi dovrebbe funzionare e in altri fallire. A questo scopo
introduciamo il concetto di &lt;em&gt;rango&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Dato un insieme finito di vettori ${{\boldsymbol v}&lt;em&gt;i}&lt;/em&gt;{i=1}^k$ di uno
spazio vettoriale $\mathbb V$, il rango dell&amp;rsquo;insieme è la dimensione di
$\mathrm{span}[{{\boldsymbol v}&lt;em&gt;i}&lt;/em&gt;{i=1}^k]$.&lt;/p&gt;
&lt;p&gt;Data una matrice ${\boldsymbol A}$ possiamo quindi assegnarle, in linea
di principio, un rango per righe, ovvero il numero massimo di sue righe
linearmenti indipendenti, e un rango per colonne, ovvero il numero
massimo di sue colonne linearmenti indipendenti. Tuttavia, vale il
seguente&lt;/p&gt;
&lt;p&gt;Il rango per righe e il rango per colonne di una matrice
${\boldsymbol A}\in{\mathbb R}^{m\times n}$ coincidono.&lt;/p&gt;
&lt;p&gt;Denotiamo d&amp;rsquo;ora in poi $r({\boldsymbol A})$ il rango di
${\boldsymbol A}$: naturalmente $r({\boldsymbol A})\leq \min{n,m}$. Il
rango di una matrice ha numerose proprietà, che non elencheremo,
eccezion fatta per la seguente&lt;/p&gt;
&lt;p&gt;Una matrice quadrata ${\boldsymbol A}\in\mathcal M_{n,n}({\mathbb K})$ è
invertibile se e solo se il suo rango è $n$.&lt;/p&gt;
&lt;p&gt;Di conseguenza, il rango di una matrice è massimo se e solo se il suo
determinante è non nullo.&lt;/p&gt;
&lt;p&gt;Il rango di una matrice rimane inalterato eseguendo le opreazioni
lineari del metodo di Gauss, ed è uguale al numero di righe non-nulle
ottenute alla fine dell&amp;rsquo;esecuazione all&amp;rsquo;interno della matrice
${\boldsymbol A}$ trasformata. Il fatto che il rango di una matrice sia
legato alla risolubilità di un sistema lineare è suggerito dal seguente
fatto. Consideriamo il problema
${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}$: se indichiamo con
${\boldsymbol a}^\nu$ la $\mu$-esima colonna di ${\boldsymbol A}$,
questo problema può scriversi come&lt;/p&gt;
&lt;p&gt;$$\sum_{\nu=1}^n x_\nu{\boldsymbol a}^\nu={\boldsymbol b},$$&lt;/p&gt;
&lt;p&gt;che esprime il fatto che stiamo cercando di scrivere ${\boldsymbol b}$
come sovrapposizione lineare dei vettori colonna di ${\boldsymbol A}$,
${{\boldsymbol a}^\nu}&lt;em&gt;{\nu=1}^n$. Ciò sarà possibile se
${\boldsymbol b}$ vive nello spazio generato dalle colonne di
${\boldsymbol A}$, $\mathrm{span}[{{\boldsymbol a}^\nu}&lt;/em&gt;{\nu=1}^n]$,
la cui dimensione è $r({\boldsymbol A})$. Questa intuizione è espressa
in termini rigorosi da un teorema fondamentale dell&amp;rsquo;algebra lineare, che
diamo senza dimostrazione.&lt;/p&gt;
&lt;p&gt;Un sistema lineare di $m$ equazioni in $n$ incognite
${\boldsymbol A}{\boldsymbol x}={\boldsymbol b}$ è compatibile se e solo
se la matrice orlata ha lo stesso rango della matrice ${\boldsymbol A}$.
In tal caso, lo spazio delle soluzioni ha dimensione
$n-r({\boldsymbol A})$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sistemi di equazioni lineari</title>
      <link>https://gsicuro.github.io/lectures/im2on/ch1/sec3/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/im2on/ch1/sec3/</guid>
      <description>&lt;p&gt;Content&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Operatori lineari e spettri</title>
      <link>https://gsicuro.github.io/lectures/im2on/ch1/sec4/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/im2on/ch1/sec4/</guid>
      <description>&lt;p&gt;Content&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meccanica Razionale</title>
      <link>https://gsicuro.github.io/lectures/mr/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/mr/</guid>
      <description>&lt;h2 id=&#34;materiale-di-riferimento&#34;&gt;Materiale di riferimento&lt;/h2&gt;
&lt;p&gt;Il testo di riferimento sarà il volume di Paolo Biscari, Tommaso Ruggeri, Giuseppe Saccomandi e Maurizio Vianello &lt;a href=&#34;https://www.amazon.it/Meccanica-razionale-138-Paolo-Biscari/dp/8847040175/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Meccanica Razionale&lt;/strong&gt;&lt;/a&gt;, edito da Springer. Un utile manuale di esercizi è la raccolta di Francesca Brini, Augusto Muracchini, Tommaso Ruggeri e Leonardo Seccia &lt;a href=&#34;https://www.amazon.it/Esercizi-desame-meccanica-razionale-espansione/dp/8893851180/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Esercizi e Temi d&amp;rsquo;Esame di Meccanica Razionale&lt;/strong&gt;&lt;/a&gt; pubblicato da Esculapio.&lt;/p&gt;
&lt;div class=&#34;alert alert-primary&#34;&gt;
  &lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;strong&gt; &lt;a href=&#34;./note.pdf&#34;&gt;Note del corso.&lt;/a&gt;&lt;/strong&gt;
&lt;/div&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://gsicuro.github.io/lectures/mr/rulletta3.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h2 id=&#34;esame&#34;&gt;Esame&lt;/h2&gt;
&lt;p&gt;Prova scritta e successiva prova orale; l&amp;rsquo;ammissione all&amp;rsquo;orale è subordinata al raggiungimento di un punteggio non inferiore a 15/30 nella prova scritta.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
  &lt;img src=&#34;voti.png&#34; alt=&#34;Voti all&#39;esame&#34;&gt;
  &lt;figcaption&gt;Voti dell&#39;esame di Meccanica Razionale ad Architettura--Ingegneria in tutti gli esami scritti dall&#39;autunno 2023. In zero appare la frazione di ritiri. Approssimativamente il 40% degli studenti raggiunge la sufficienza. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;prove-passate&#34;&gt;Prove passate&lt;/h3&gt;
&lt;p&gt;Le prove sono riportate con relativa soluzione.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;./provatest.pdf&#34;&gt;&lt;strong&gt;Prova esempio&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;2024&#34;&gt;2024&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;09-09-2024&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;15-07-2024&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;01-07-2024&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240617.pdf&#34;&gt;&lt;strong&gt;17-06-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240322.pdf&#34;&gt;&lt;strong&gt;22-03-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240207.pdf&#34;&gt;&lt;strong&gt;07-02-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240117.pdf&#34;&gt;&lt;strong&gt;17-01-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2023&#34;&gt;2023&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;./20231201.pdf&#34;&gt;&lt;strong&gt;01-12-2023&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://gsicuro.github.io/lectures/mr/rulletta2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Istituzioni di Matematica 2</title>
      <link>https://gsicuro.github.io/lectures/im2/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/im2/</guid>
      <description>&lt;div class=&#34;alert alert-secondary&#34;&gt;
&lt;small&gt; Materiale per il modulo di &lt;i&gt;Istituzioni di Matematica 2&lt;/i&gt; del corso di laurea in Architettura-Ingegneria dell&#39;Università di Bologna durante l&#39;anno accademico 2023-2024.&lt;/small&gt;
&lt;/div&gt;
&lt;h2 id=&#34;dispense&#34;&gt;Dispense&lt;/h2&gt;
&lt;p&gt;Il corso consiste di due parti. La prima si focalizza su elementi dell&amp;rsquo;algebra lineare, mentre la seconda tratta i fondamenti del calcolo in molte variabili.&lt;/p&gt;
&lt;div class=&#34;alert alert-primary&#34;&gt;
  &lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;strong&gt; &lt;a href=&#34;./note.pdf&#34;&gt;Note del corso.&lt;/a&gt;&lt;/strong&gt;
&lt;/div&gt;
Si noti che le note possono contenere errori.
&lt;h2 id=&#34;esame&#34;&gt;Esame&lt;/h2&gt;
&lt;p&gt;Prova scritta e successiva prova orale; l&amp;rsquo;ammissione all&amp;rsquo;orale è subordinata al raggiungimento di un punteggio non inferiore a 15/30 nella prova scritta.&lt;/p&gt;
&lt;h3 id=&#34;prove-passate&#34;&gt;Prove passate&lt;/h3&gt;
&lt;p&gt;Le prove sono riportate con relativa soluzione.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10-09-2024&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;02-07-2024&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240618.pdf&#34;&gt;&lt;strong&gt;18-06-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240212.pdf&#34;&gt;&lt;strong&gt;12-02-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240131.pdf&#34;&gt;&lt;strong&gt;31-01-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./20240115.pdf&#34;&gt;&lt;strong&gt;15-01-2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Analytical Approaches ​for Neural Network Dynamics</title>
      <link>https://gsicuro.github.io/news/dynparis2023/</link>
      <pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/dynparis2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://gsicuro.github.io/publications/</link>
      <pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/publications/</guid>
      <description>&lt;iframe type=&#34;text/html&#34; sandbox=&#34;allow-scripts allow-same-origin allow-popups&#34; width=&#34;336&#34; height=&#34;550&#34; frameborder=&#34;0&#34; allowfullscreen style=&#34;max-width:45&#34; src=&#34;https://read.amazon.com/kp/card?asin=B01M4NFA51&amp;preview=inline&amp;linkCode=kpe&amp;ref_=cm_sw_r_kb_dp_NT1Y2ZQ2WYTV618DW936&#34; &gt;&lt;/iframe&gt;
&lt;iframe type=&#34;text/html&#34; sandbox=&#34;allow-scripts allow-same-origin allow-popups&#34; width=&#34;336&#34; height=&#34;550&#34; frameborder=&#34;0&#34; allowfullscreen style=&#34;max-width:45%&#34; src=&#34;https://read.amazon.com/kp/card?asin=B0CFLFFSNH&amp;preview=inline&amp;linkCode=kpe&amp;ref_=cm_sw_r_kb_dp_2M07EK3QZZGHYBF6J6DK&#34; &gt;&lt;/iframe&gt;
&lt;hr&gt;
&lt;html&gt;
&lt;head&gt;
&lt;script type=&#34;text/javascript&#34;&gt;
var arxiv_authorid = &#34;0000-0002-9258-2436&#34;;
var arxiv_format=&#34;arxiv&#34;;
var arxiv_includeSubjects=0;
var arxiv_max_entries=0;
&lt;/script&gt;
&lt;style type=&#34;text/css&#34;&gt;
div.arxivfeed {margin-bottom: 5px; width:90%;}
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;arxiv.js&#34;&gt;
&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&#34;arxivfeed&#34;&gt;&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title>Disordered Systems Days at King&#39;s College London</title>
      <link>https://gsicuro.github.io/news/kuhn2023/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/kuhn2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spin Glass Theory &amp; Far Beyond</title>
      <link>https://gsicuro.github.io/news/rsb40/</link>
      <pubDate>Fri, 30 Jun 2023 10:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/rsb40/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stochastic Processes and applications</title>
      <link>https://gsicuro.github.io/lectures/ps/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/lectures/ps/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://gsicuro.github.io/uploads/7CCMCS04.pdf&#34;&gt;&lt;strong&gt;These lecture notes&lt;/strong&gt;&lt;/a&gt; are largely based on a previous version prepared by A. Annibale, for the &lt;em&gt;Dynamical analysis of complex systems&lt;/em&gt;, later named &lt;em&gt;Stochastic processes and applications&lt;/em&gt;, module at King&amp;rsquo;s College London. The module is part of the &lt;em&gt;Complex Systems Modelling&lt;/em&gt; master and I taught it from 2021 to 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Facets of Statistical Field Theory</title>
      <link>https://gsicuro.github.io/news/caracciolo2022/</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/caracciolo2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MECO47</title>
      <link>https://gsicuro.github.io/news/meco47/</link>
      <pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/meco47/</guid>
      <description></description>
    </item>
    
    <item>
      <title>73rd British Mathematical Colloquium</title>
      <link>https://gsicuro.github.io/news/bmc2022/</link>
      <pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/bmc2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Current and past students</title>
      <link>https://gsicuro.github.io/alumni/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/alumni/</guid>
      <description>&lt;h2 id=&#34;phd-students&#34;&gt;PhD students&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;From 2021, &lt;strong&gt;Urte Adomaityte&lt;/strong&gt;. King&amp;rsquo;s College London.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;master-students&#34;&gt;Master students&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2016, &lt;strong&gt;Matteo P. D&amp;rsquo;Achille&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/dachille.pdf&#34;&gt;&lt;em&gt;On two assignment problems&lt;/em&gt;&lt;/a&gt;, co-supervised with Sergio Caracciolo, University of Milan.&lt;/li&gt;
&lt;li&gt;2018, &lt;strong&gt;Gianmarco Perrupato&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/perrupato.pdf&#34;&gt;&lt;em&gt;Study of matching on the Bethe lattice&lt;/em&gt;&lt;/a&gt;, co-supervised with Giorgio Parisi, Sapienza University of Rome.&lt;/li&gt;
&lt;li&gt;2021, &lt;strong&gt;Anshul Toshniwal&lt;/strong&gt;. Thesis: &lt;em&gt;The planted multi-index matching problem&lt;/em&gt;, co-supervised with Lenka Zdeborová, EPFL.&lt;/li&gt;
&lt;li&gt;2021, &lt;strong&gt;Claudia De Sousa Miranda Perez&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/desousa.pdf&#34;&gt;&lt;em&gt;The random dimer covering problem on the weighted Aztec graph&lt;/em&gt;&lt;/a&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2021, &lt;strong&gt;Daniel Reti&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/reti.pdf&#34;&gt;&lt;em&gt;Robustness of excitations in the random dimer model&lt;/em&gt;&lt;/a&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2021, &lt;strong&gt;Hugo Ryder&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/ryder.pdf&#34;&gt;&lt;em&gt;Matching recovery&lt;/em&gt;&lt;/a&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2021, &lt;strong&gt;Leonardo Scialo&lt;/strong&gt;. Thesis: &lt;a href=&#34;https://gsicuro.github.io/uploads/scialo.pdf&#34;&gt;&lt;em&gt;Arctic region in the weighted random dimer covering&lt;/em&gt;&lt;/a&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2022, &lt;strong&gt;Guoyu Chang&lt;/strong&gt;. Thesis: &lt;em&gt;The mixed multi-index matching problem&lt;/em&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2022, &lt;strong&gt;Xiaoying Zhou&lt;/strong&gt;. Thesis: &lt;em&gt;Statistical physics of community detection&lt;/em&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;li&gt;2023, &lt;strong&gt;Wenjuan Li&lt;/strong&gt;. Thesis: &lt;em&gt;Stable Marriage Problems: Perturbations and Correlations&lt;/em&gt;, King&amp;rsquo;s College London.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>40 years of Replica Symmetry Breaking</title>
      <link>https://gsicuro.github.io/news/confrsb40/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/confrsb40/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Disordered serendipity</title>
      <link>https://gsicuro.github.io/news/parisi70/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/parisi70/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Beyond mean field theory</title>
      <link>https://gsicuro.github.io/news/bmft/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/news/bmft/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://gsicuro.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gsicuro.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
