[{"authors":null,"categories":null,"content":"Curriculum Vitae 2006\u0026ndash;2011 BSc and MSc in Physics at the Università del Salento in Lecce. 2012\u0026ndash;2015 PhD at the Università di Pisa, under the supervision of Sergio Caracciolo. 2015\u0026ndash;2016 Postdoctoral researcher in Rio de Janeiro at CBPF in the group of Constantino Tsallis. 2017\u0026ndash;2019 Postdoctoral researcher at Sapienza Università di Roma, in the Chimera group led by Giorgio Parisi. 2020 Postdoctoral researcher at ENS in Paris and EPFL in Lausanne in the IdePHICS group led by Florent Krzakala. 2021\u0026ndash;2023 Lecturer in the Disordered systems Group, in the Mathematics Department at King\u0026rsquo;s College London. From 2023 Associate professor in Mathematical Physics in the Mathematics Department at University of Bologna. ","date":1704067200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1704067200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gsicuro.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Curriculum Vitae 2006\u0026ndash;2011 BSc and MSc in Physics at the Università del Salento in Lecce. 2012\u0026ndash;2015 PhD at the Università di Pisa, under the supervision of Sergio Caracciolo. 2015\u0026ndash;2016 Postdoctoral researcher in Rio de Janeiro at CBPF in the group of Constantino Tsallis.","tags":null,"title":"Gabriele Sicuro","type":"authors"},{"authors":null,"categories":null,"content":"Vettori applicati e vettori geometrici Per introdurre il concetto di spazio vettoriale iniziamo da un contesto più concreto e forse più semplice. Consideriamo due punti $A$ e $B$ nello spazio ordinario. Un vettore applicato ${\\boldsymbol v}=\\overrightarrow{AB}$ in $A$ con punto finale $B$ può essere immaginato come una freccia che collega $A$ e $B$,\nUn vettore applicato è caratterizzato dal suo punto di applicazione, $A$, da una direzione, un verso e una lunghezza. Se $A=B$, allora il corrispondente vettore è detto nullo e viene indicato con $\\mathbf 0$: la sua lunghezza è zero e la sua direzione e il suo verso non sono definiti. Due vettori ${\\boldsymbol v}=\\overrightarrow{AB}$ e ${\\boldsymbol u}=\\overrightarrow{CD}$ sono equipollenti se hanno stessa direzione, stessa lunghezza e stesso verso. L\u0026rsquo;equipollenza è una relazione di equivalenza e un vettore geometrico corrisponde ad una classe di equivalenza secondo equipollenza. Uno specifico vettore applicato è perciò un rappresentante di questa classe di equivalenza, mentre tutti i vettori con stessa direzione, stessa lunghezza e stesso verso corrispondono allo stesso oggetto di tipo \u0026ldquo;vettore geometrico\u0026rdquo;.\nOperazioni tra vettori geometrici I vettori geometrici sono oggetti molto diversi dai numeri usuali, ma è ugualmente possibile introdurre una serie di operazioni tra di essi.\nDati due vettori geometrici ${\\boldsymbol v}$ e ${\\boldsymbol u}$, la loro somma ${\\boldsymbol u}+{\\boldsymbol v}$ può essere definita concatenando due loro rappresentanti. Per esempio, scelto un punto $A$, si applica prima ${\\boldsymbol v}$ ad $A$ si ottiene il vettore $\\overrightarrow{AB}$ con coda in un punto $B$, e, consecutivamente, applicando ${\\boldsymbol u}$ a $B$ si ottiene il vettore $\\overrightarrow{BC}$ con coda in $C$. Possiamo quindi definire $\\overrightarrow{AC}$ come rappresentante del vettore geometrico somma ${\\boldsymbol v}+{\\boldsymbol u}$. La costruzione ora descritta corrisponde ala cosiddetta regola del parallelogramma, ed è equivalente a considerare ${\\boldsymbol u}$ e ${\\boldsymbol v}$ applicati allo stesso punto $A$ e costrure su di essi un parallelogramma, considerando quindi come somma la diagonale del quadrilatero ottenuto:\nÈ evidente dalla costruzione stessa che l\u0026rsquo;operazione è commutativa, ovvero ${\\boldsymbol v}+{\\boldsymbol u}={\\boldsymbol u}+{\\boldsymbol v}$, e tale per cui ${\\boldsymbol v}+\\mathbf 0={\\boldsymbol v}$. L\u0026rsquo;operazione di somma che abbiamo definito è anche associativa. Se abbiamo tre vettori ${\\boldsymbol v}$, ${\\boldsymbol u}$ e ${\\boldsymbol w}$, allora\nInfine, dato un vettore applicato $\\overrightarrow{AB}$, il vettore applicato $\\overrightarrow{BA}$ è tale per cui $\\overrightarrow{AB}+\\overrightarrow{BA}=\\mathbf 0$: se il vettore geometrico associato a $\\overrightarrow{AB}$ è ${\\boldsymbol v}$, denotiamo quindi $-{\\boldsymbol v}$ il vettore geometrico associato a $\\overrightarrow{BA}$ e scriviamo ${\\boldsymbol v}+(-{\\boldsymbol v})=\\mathbf 0$: $-{\\boldsymbol v}$ è l\u0026rsquo;opposto di ${\\boldsymbol v}$.\nÈ possibile anche introdurre l\u0026rsquo;operazione moltiplicazione per uno scalare $c\\in{\\mathbb R}$: il vettore $c{\\boldsymbol v}$ ha stessa direzione di ${\\boldsymbol v}$, stesso verso se $c\u0026gt;0$ e verso opposto se $c\u0026lt;0$, e lunghezza uguale a $|c|$ volte quella di ${\\boldsymbol v}$.\nSe $c=0$, allora $c{\\boldsymbol v}=\\mathbf 0$. È facile verificare che, se $a,b\\in{\\mathbb R}$, allora $(a+b){\\boldsymbol v}=a{\\boldsymbol v}+b{\\boldsymbol v}$, $(ab){\\boldsymbol v}=a(b{\\boldsymbol v})$ e, dati due vettori ${\\boldsymbol v}$ e ${\\boldsymbol u}$, $a({\\boldsymbol v}+{\\boldsymbol u})=a{\\boldsymbol v}+a{\\boldsymbol u}$.\nSpazi vettoriali Lo spazio dei vettori geometrici può avere una interpretazione intuitiva abbastanza semplice. Può rappresentare, per esempio, lo spazio degli spostamenti nel piano. Tuttavia la sua struttura può essere generalizzata in forma astratta: essa è caratterizzata dalla presenza di due operazioni binarie (somma tra vettori e prodotto per scalare) e lo spazio vettoriale dei vettori geometrici è stato introdotto proprio per mezzo di queste operazioni.\nL\u0026rsquo;idea ora è tentare di caratterizzare cosa sia uno spazio vettoriale senza necessariamente avere in mente una rappresentazione pittorica. In generale, la costruzione di uno spazio vettoriale richiede l\u0026rsquo;esistenza di un insieme $\\mathbb V$ di \u0026ldquo;vettori\u0026rdquo; e di un altro insieme di \u0026ldquo;scalari\u0026rdquo;, ${\\mathbb K}$. Ci limiteremo ad assumere che ${\\mathbb K}$ sia, a seconda dei casi ${\\mathbb Q}$, ${\\mathbb R}$ o ${\\mathbb C}$, ma utilizzeremo ${\\mathbb K}$ per lavorare in maggiore generalità.\nIl concetto di spazio vettoriale può quindi essere formalizzato come segue.\nDefinizione Uno spazio vettoriale su un campo ${\\mathbb K}$ è un insieme non vuoto $\\mathbb V$ tale per cui esistono due operazioni binarie, ovvero $+\\colon\\mathbb V\\times\\mathbb V\\to\\mathbb V$, detta somma, e $\\cdot \\colon{\\mathbb K}\\times\\mathbb V\\to\\mathbb V$, detta prodotto per uno scalare, con le seguenti proprietà: la somma $+$ è commutativa e associativa; esiste un elemento neutro $\\mathbf 0$, ovvero se ${\\boldsymbol v}\\in\\mathbb V$, allora ${\\boldsymbol v}+\\mathbf 0=\\mathbf 0+{\\boldsymbol v}={\\boldsymbol v}$; esiste l\u0026rsquo;elemento opposto, ovvero se ${\\boldsymbol v}\\in\\mathbb V$ esiste $-{\\boldsymbol v}\\in\\mathbb V$ tale per cui ${\\boldsymbol v}+(-{\\boldsymbol v})=\\mathbf 0$;\nil prodotto $\\cdot$ è distributivo sulla somma di vettori (ovvero $a({\\boldsymbol v}+{\\boldsymbol u})=a{\\boldsymbol v}+a{\\boldsymbol u}$ per $a\\in{\\mathbb K}$ e ${\\boldsymbol v},{\\boldsymbol u}\\in\\mathbb V$) e sulla somma di scalari (ovvero $(a+b){\\boldsymbol v}=a{\\boldsymbol v}+b{\\boldsymbol v}$ per $a,b\\in{\\mathbb K}$ e ${\\boldsymbol v}\\mathbb V$); infine l\u0026rsquo;elemento neutro $1$ di ${\\mathbb K}$ è tale per cui $1{\\boldsymbol v}={\\boldsymbol v}$.\nLa definizione precedente si applica in effetti ai vettori geometrici, ed è anzi ispirata da essi. Come potete notare, è richiesto che ${\\mathbb K}$ sia un campo: come anticipato, per semplicità assumeremo che ${\\mathbb K}$ sia un campo numerico come ${\\mathbb R}$, ${\\mathbb Q}$ e ${\\mathbb C}$.\nUn esempio di spazio vettoriale è dato dai vettori nel piano $\\mathbb V={\\mathbb R}^2$. Un vettore nel piano ${\\boldsymbol x}$ è rappresentato da una freccia applicata nell'origine che punta in una posizione del piano e può rappresentarsi come una coppia di numeri come $${\\boldsymbol x}= \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix} \\in{\\mathbb R}^2,$$ corrispondenti alle sue coordinate. Dati due vettori ${\\boldsymbol x}_1$ e ${\\boldsymbol x}_2$, una loro combinazione lineare con coefficienti reali $c_1$ e $c_2$ si ottiene come $${\\boldsymbol x}_1= \\begin{pmatrix} x_{11}\\\\x_{12} \\end{pmatrix} ,\\quad {\\boldsymbol x}_2= \\begin{pmatrix} x_{21}\\\\x_{22} \\end{pmatrix} \\Rightarrow c_1{\\boldsymbol x}_1+c_2{\\boldsymbol x}_2= \\begin{pmatrix} c_1x_{11}+c_2x_{21}\\\\c_1x_{12}+c_2x_{22} \\end{pmatrix}.$$ Per esempio, nella figura sotto ${\\boldsymbol x}_1=\\binom{3}{1}$ e ${\\boldsymbol x}_2=\\binom{1}{4}$, ed in rosso è rappresentata la loro somma $2{\\boldsymbol x}_1+{\\boldsymbol x}_2=\\binom{7}{6}$.\nSe ${\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_k\\in\\mathbb V$ e $c_1,\\dots,c_k\\in{\\mathbb K}$, il vettore ${\\boldsymbol v}=c_1{\\boldsymbol v}_1+\\dots+c_k{\\boldsymbol v}_k\\in\\mathbb V$ si dice essere una combinazione lineare di ${\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_k$ a coefficienti $c_1,\\dots,c_k$. Il concetto di combinazione lineare è cruciale e caratterizza le proprietà di uno spazio vettoriale. Con un leggero abuso, utilizzeremo la notazione\n$$c_1{\\boldsymbol v}_1+\\dots+c_k{\\boldsymbol v}_k\\equiv \\sum_{i=1}^kc_i{\\boldsymbol v}_i.$$\nChiamiamo l\u0026rsquo;insieme di tutte le combinazioni lineari dei vettori $\\mathcal V\\equiv \\{{\\boldsymbol v}_i\\}_{i=1}^k$ lo span di $\\{{\\boldsymbol v}_i\\}_{i=1}^k$ e scriviamo $\\mathrm{span}(\\mathcal V)$.\nDefinizione I vettori $\\{{\\boldsymbol v}_i\\}_{i=1}^k$ sono linearmente dipendenti se esistono degli scalari $\\{c_i\\}_{i=1}^k$ non tutti nulli tali per cui $$\\sum_{i=1}^kc_i{\\boldsymbol v}_i=\\mathbf 0;$$\ndiversamente si dicono linearmente indipendenti.\nPer definizione, il singolo vettore ${\\boldsymbol v}$ è linearmente dipendente se e solo se uguale al vettor nullo. Viceversa, se ${\\boldsymbol v}_2=c{\\boldsymbol v}_1$, ${\\boldsymbol v}_1$ e ${\\boldsymbol v}_2$ sono linearmente dipendenti. La definizione implica che in un insieme di vettori linearmente dipendenti, almeno uno di essi può sempre esprimersi come combinazione lineare degli altri. Infine, vale la seguente proposizione.\nProposizione Se $\\{{\\boldsymbol v}_i\\}_{i=1}^k$ sono linearmente indipendenti, allora, dati due set di scalari in ${\\mathbb K}$ $\\{a_i\\}_{i=1}^k$ e $\\{b_i\\}_{i=1}^k$ $$\\sum_{i=1}^ka_i{\\boldsymbol v}_i=\\sum_{i=1}^kb_i{\\boldsymbol v}_i\\Rightarrow a_i=b_i\\ \\forall i=1,\\dots,k.$$ Infine, possiamo introdurre il concetto di base\nDefinizione Il set $\\mathcal B\\equiv \\{{\\boldsymbol v}_i\\}_{i=1}^n$ di vettori di $\\mathbb V$ è una base se essi sono linearmente indipendenti e se $\\mathbb V=\\mathrm{span}[\\mathcal B]$. Questo significa che ogni vettore ${\\boldsymbol v}\\in\\mathbb V$ può scriversi in maniera unica come ${\\boldsymbol v}=\\sum_{i=1}^n c_i{\\boldsymbol v}_i$, con coefficienti $c_i$ che sono detti coordinate di ${\\boldsymbol v}$ secondo la base $\\mathcal B$. Ciò comporta anche che non è possibile avere un set di vettori indipendenti con più di $n$ elementi. Vale infatti il seguente\nTeorema Sia $\\mathcal B\\equiv \\{{\\boldsymbol v}_i\\}_{i=1}^n$ una base di $\\mathbb V$. Allora ogni set $\\{{\\boldsymbol u}_i\\}_{i=1}^m$ di $m\u003en$ vettori di $\\mathbb V$ è costituito da elementi linearmente dipendenti. Di conseguenza, ogni base di $\\mathbb V$ ha $n$ elementi: il numero $n$ prende il nome di dimensione di $\\mathbb V$. Lo spazio vettoriale ${\\mathbb R}^n$ su ${\\mathbb R}$ è costituito da vettori $n$-dimensionali, rappresentabili come vettori colonna, $${\\boldsymbol x}=\\begin{pmatrix}x_1\\\\ \\vdots\\\\x_n\\end{pmatrix}.$$\nIn questo caso è facilmente identificata una base canonica $\\mathcal B=\\{{\\boldsymbol e}_i\\}_{i=1}^n$ di $n$ vettori\n$${\\boldsymbol e}_1\\equiv \\begin{pmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix} \\quad{\\boldsymbol e}_2\\equiv \\begin{pmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{pmatrix} \\quad\\dots \\quad{\\boldsymbol e}_n\\equiv \\begin{pmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{pmatrix} $$\nche permette di scrivere in maniera univoca qualunque vettore ${\\boldsymbol x}\\in{\\mathbb R}^n$ come\n$${\\boldsymbol x}=x_1 {\\boldsymbol e}_1+\\dots+ x_n{\\boldsymbol e}_n.$$\nSottospazi vettoriali Dato uno spazio vettoriale $\\mathbb V$, è possibile identificare in alcuni casi un sottospazio vettoriale ${\\mathbb W}$, ovvero un sottoinsieme di $\\mathbb V$ che è chiuso sotto le operazioni di somma e prodotto per uno scalare, ovvero\nDefinizione Dato uno ${\\mathbb K}$-spazio vettoriale $\\mathbb V$, un sottoinsieme non vuoto $\\mathbb W\\subset\\mathbb V$ è un sottospazio vettoriale di $\\mathbb V$ se, per ogni ${\\boldsymbol w},{\\boldsymbol w}'\\in\\mathbb W$, ${\\boldsymbol w}+{\\boldsymbol w}'\\in\\mathbb W$, e per ogni $c\\in{\\mathbb K}$, se ${\\boldsymbol w}\\in\\mathbb W$ allora $c{\\boldsymbol w}\\in\\mathbb W$. Per esempio, se $\\mathcal V\\subset\\mathbb V$ è un sottoinsieme finito di $\\mathbb V$, $\\mathrm{span}[\\mathcal V]$ è un sottospazio di $\\mathbb V$ (in particolare, lo è anche per $\\mathcal V$ ha cardinalità 1). Inoltre la dimensione di un sottospazio è sempre limitata superiormente dalla dimensione dello spazio in cui vive. È interessante notare che se $\\mathbb U$ e $\\mathbb W$ sono sottospazi vettoriali di $\\mathbb V$, allora $\\mathbb U\\cap\\mathbb W$ è anche sottospazio vettoriale di $\\mathbb V$ (mentre la loro unione, in generale, non lo è).\nSe $\\mathbb V=\\mathbb R^2$, per esempio, dato un vettore non nullo $\\boldsymbol v\\in\\mathbb R^2$, un sottospazio vettoriale può costruirsi semplicemente considerando $\\mathbb W=\\{\\boldsymbol w\\in\\mathbb V\\colon \\boldsymbol w=c\\boldsymbol v,\\ c\\in\\mathbb R\\}$. Esso corrisponde ad una retta lungo la direzione di $\\boldsymbol v$. Dati due sottospazi vettoriali $\\mathbb U$ e $\\mathbb W$, se $\\mathbb U\\cap\\mathbb W={\\mathbf 0}$ è possibile costruire un nuovo spazio, detto somma diretta di $\\mathbb U$ e $\\mathbb W$ e indicato con $\\mathbb U\\oplus\\mathbb W$. Questo spazio consiste di tutti i vettori ${\\boldsymbol v}$ nella forma ${\\boldsymbol v}={\\boldsymbol u}+{\\boldsymbol w}$ con ${\\boldsymbol u}\\in\\mathbb U$, ${\\boldsymbol w}\\in\\mathbb W$. Inoltre questa decomposizione è unica: se infatti assumessimo che esiste un\u0026rsquo;altra coppia tale per cui ${\\boldsymbol v}={\\boldsymbol u}\u0026rsquo;+{\\boldsymbol w}\u0026rsquo;$, con ${\\boldsymbol u}\u0026rsquo;\\in\\mathbb U$, ${\\boldsymbol w}\u0026rsquo;\\in\\mathbb W$, allora dovremmo avere ${\\boldsymbol u}+{\\boldsymbol w}={\\boldsymbol u}\u0026rsquo;+{\\boldsymbol w}\u0026rsquo;$ e quindi ${\\boldsymbol u}-{\\boldsymbol u}\u0026rsquo;={\\boldsymbol w}-{\\boldsymbol w}\u0026rsquo;$. Ma questa quantità può essere solo $\\mathbf 0$, unico elemento nell\u0026rsquo;intersezione dei due spazi.\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"a8930feb8b6bca267e6508618ec2ab5f","permalink":"https://gsicuro.github.io/lectures/im2on/ch1/sec1/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/lectures/im2on/ch1/sec1/","section":"lectures","summary":"Vettori applicati e vettori geometrici Per introdurre il concetto di spazio vettoriale iniziamo da un contesto più concreto e forse più semplice. Consideriamo due punti $A$ e $B$ nello spazio ordinario.","tags":null,"title":"Spazi vettoriali","type":"book"},{"authors":null,"categories":null,"content":"Matrici Un esempio particolarmente importante e prototipico di spazio vettoriale è costituito dallo spazio delle matrici di elementi in ${\\mathbb K}$. Consideriamo ora due interi positivi $m$ e $n$. Una matrice di $m\\times n$ elementi in ${\\mathbb K}$ è una tabella del tipo\n$${\\boldsymbol A}= \\begin{pmatrix}a_{11}\u0026amp;a_{12}\u0026amp;\\dots\u0026amp;a_{1n}\\\\ a_{21}\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{2n}\\\\ \\cdots\u0026amp;\\cdots\u0026amp;\\ddots\u0026amp;\\vdots\\\\ a_{m1}\u0026amp;a_{m2}\u0026amp;\\dots\u0026amp;a_{mn} \\end{pmatrix} $$\ndove, $\\forall \\mu,\\nu$, $a_{\\mu\\nu}\\in{\\mathbb K}$: il primo indice denota la riga dell\u0026rsquo;elemento nella tabella, il secondo la sua colonna. Scriveremo ${\\boldsymbol A}=(a_{\\mu\\nu})_{\\mu\\nu}\\in\\mathcal M_{m,n}({\\mathbb K})$. A volte si denota\n$${\\boldsymbol a}_\\mu=(a_{\\mu 1},\\dots, a_{\\mu n})$$\nla $\\mu$-esima riga e\n$${\\boldsymbol a}^\\nu=\\begin{pmatrix}a_{1\\nu}\\\\ \\dots\\\\a_{m\\nu} \\end{pmatrix} $$\nla $\\nu$-esima colonna. Ogni matrice ${\\boldsymbol A}$ può essere associata ad una matrice ${\\boldsymbol A}^\\intercal\\in\\mathcal M_{n,m}({\\mathbb K})$ detta trasposta ottenuta invertendo righe con colonne:\n$${\\boldsymbol A}= \\begin{pmatrix}a_{11}\u0026amp;a_{12}\u0026amp;\\dots\u0026amp;a_{1n}\\\\ a_{21}\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{2n}\\\\ \\cdots\u0026amp;\\cdots\u0026amp;\\ddots\u0026amp;\\vdots\\\\ a_{m1}\u0026amp;a_{m2}\u0026amp;\\dots\u0026amp;a_{mn} \\end{pmatrix} \\Rightarrow {\\boldsymbol A}^\\intercal= \\begin{pmatrix}a_{11}\u0026amp;a_{21}\u0026amp;\\dots\u0026amp;a_{m1}\\\\ a_{12}\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{m2}\\\\ \\cdots\u0026amp;\\cdots\u0026amp;\\ddots\u0026amp;\\vdots\\\\ a_{1n}\u0026amp;a_{2n}\u0026amp;\\dots\u0026amp;a_{mn} \\end{pmatrix} $$\nL\u0026rsquo;operazione di trasposizione è quindi tale che $\\intercal\\colon\\mathcal M_{n,m}({\\mathbb K})\\to \\mathcal M_{m,n}({\\mathbb K})$. Matrici con $n=1$ sono in particolare dette vettori colonna, e viene tipicamente usata la notazione $\\mathcal M_{m,1}({\\mathbb K})\\equiv {\\mathbb K}^m$. Lo spazio delle matrici su $\\mathcal M_{m,n}({\\mathbb K})$ costituisce uno spazio vettoriale se introduciamo le seguenti operazioni. Date due matrici ${\\boldsymbol A}=(a_{\\mu\\nu}){\\mu\\nu},{\\boldsymbol B}=(b{\\mu\\nu}){\\mu\\nu}\\in \\mathcal M{m,n}({\\mathbb K})$, e uno scalare $c\\in{\\mathbb K}$, allora definiamo\n$${\\boldsymbol A}+{\\boldsymbol B}\\equiv (a_{\\mu\\nu}+b_{\\mu\\nu}){\\mu\\nu}\\in \\mathcal M{m,n}({\\mathbb K}),\\qquad c{\\boldsymbol A}\\equiv (ca_{\\mu\\nu}){\\mu\\nu}\\in\\mathcal M{m,n}({\\mathbb K}).$$\nQueste operazioni soddisfano le proprietà richieste per rendere $\\mathcal M_{m,n}({\\mathbb K})$ uno spazio vettoriale secondo la Definizione [def:spaziovettoriale]{reference-type=\u0026ldquo;ref\u0026rdquo; reference=\u0026ldquo;def:spaziovettoriale\u0026rdquo;}. Questo spazio ha dimensione $mn$. Una base è costuita dalle matrici ${\\boldsymbol E}{ij}=(\\delta{i\\mu}\\delta_{j\\nu})_{\\mu\\nu}$.\nProdotto tra matrici È possibile definire ora una nuova operazione, quella di prodotto tra matrici. Consideriamo una matrice ${\\boldsymbol A}\\in\\mathcal M_{n,k}({\\mathbb K})$ e una matrice ${\\boldsymbol B}\\in\\mathcal M_{k,m}({\\mathbb K})$. Allora è possibile introdurre una matrice ${\\boldsymbol C}=(c_{\\mu\\nu}){\\mu\\nu}={\\boldsymbol A}{\\boldsymbol B}\\in\\mathcal M{n,m}({\\mathbb K})$ tale che\n$$c_{\\mu\\nu}=\\sum_{\\rho=1}^ka_{\\mu\\rho}b_{\\rho\\nu}.$$\nIn particolare, se ${\\boldsymbol u}$ è un $n$-vettore riga\n$${\\boldsymbol u}= \\begin{pmatrix} u_1\u0026amp;\\dots\u0026amp; u_n \\end{pmatrix} $$\ne ${\\boldsymbol v}$ un $n$-vettore colonna,\n$${\\boldsymbol v}= \\begin{pmatrix} v_1\\\\dots\\ v_n \\end{pmatrix} $$\nè possibile definire il prodotto scalare tra i due come\n$${\\boldsymbol u}{\\boldsymbol v}=\\sum_{\\nu=1}^n u_\\nu v_\\nu.$$\nNel caso dei vettori colonna, in particolare, si indica con $|{\\boldsymbol v}|^2\\equiv {\\boldsymbol v}^\\intercal{\\boldsymbol v}=\\sum_{\\nu=1}^nv_\\nu^2$ la loro norma.\nNotare che l\u0026rsquo;operazione di prodotto tra matrici non è abeliana e anzi se $n\\neq m$ il prodotto ${\\boldsymbol B}{\\boldsymbol A}$ è non definito. Anche qualora l\u0026rsquo;operazione fosse possibile, per esempio se $n=m$, ${\\boldsymbol A}{\\boldsymbol B}\\neq {\\boldsymbol B}{\\boldsymbol A}$ in generale.\nLe proprietà del prodotto tra matrici sono date dalla seguente Proposizione, la cui dimostrazione avviene controllando esplicitamente la validità di ogni affermazione.\nSiano ${\\boldsymbol A},{\\boldsymbol B}\\in\\mathcal M_{m,n}({\\mathbb K})$ e ${\\boldsymbol C},{\\boldsymbol D}\\in\\mathcal M_{n,p}({\\mathbb K})$, e sia $c\\in{\\mathbb K}$. Allora\n$({\\boldsymbol A}+{\\boldsymbol B}){\\boldsymbol C}={\\boldsymbol A}{\\boldsymbol C}+{\\boldsymbol B}{\\boldsymbol C}$;\n${\\boldsymbol A}({\\boldsymbol C}+{\\boldsymbol D})={\\boldsymbol A}{\\boldsymbol C}+{\\boldsymbol A}{\\boldsymbol D}$;\n${\\boldsymbol A}(c{\\boldsymbol C})=c({\\boldsymbol A}{\\boldsymbol C})=(c{\\boldsymbol A}){\\boldsymbol C}$;\n${\\boldsymbol A}{\\boldsymbol I}_n={\\boldsymbol A}={\\boldsymbol I}_m{\\boldsymbol A}$;\n$({\\boldsymbol A}{\\boldsymbol C})^\\intercal={\\boldsymbol C}^\\intercal{\\boldsymbol A}^\\intercal$;\n$({\\boldsymbol A}+{\\boldsymbol B})^\\intercal={\\boldsymbol A}^\\intercal+{\\boldsymbol B}^\\intercal$;\n$({\\boldsymbol A}{\\boldsymbol C}){\\boldsymbol Q}={\\boldsymbol A}({\\boldsymbol C}{\\boldsymbol Q})$ se ${\\boldsymbol Q}\\in{\\mathbb K}^{p\\times q}$.\nMatrici quadrate Data una matrice ${\\boldsymbol A}\\in\\mathcal M_{m,n}({\\mathbb K})$, se $m=n$ la matrice ${\\boldsymbol A}$ è detta quadrata di dimensione $n$. Una matrice quadrata è diagonale se ha la forma ${\\boldsymbol A}=(a_{\\mu}\\delta_{\\mu\\nu}){\\mu\\nu}$, dove $\\delta{\\mu\\nu}$ è il simbolo di Kronecker. Tra le matrici diagonali la matrice unità ${\\boldsymbol I}n\\equiv (\\delta{\\mu\\nu})_{\\mu\\nu}$ ha un ruolo speciale dato che si comporta come l\u0026rsquo;identità rispetto al prodotto tra matrici, come vedremo. Infine, una matrice quadrata ${\\boldsymbol A}\\in{\\mathbb K}$ è simmetrica se ${\\boldsymbol A}={\\boldsymbol A}^\\intercal$ e antisimmetrica se ${\\boldsymbol A}=-{\\boldsymbol A}^\\intercal$. Elenchiamo ora alcune proprietà delle matrici quadrate.\nUna matrice quadrata ${\\boldsymbol A}\\in\\mathcal M_{n,n}({\\mathbb K})$ si dice invertibile se esiste una matrice quadrata $n\\times n$, che denotiamo con ${\\boldsymbol A}^{-1}$, tale che ${\\boldsymbol A}{\\boldsymbol A}^{-1}={\\boldsymbol A}^{-1}{\\boldsymbol A}={\\boldsymbol I}_n$.\nSia ${\\boldsymbol A}\\in\\mathcal M_{n,n}({\\mathbb K})$. La sua matrice inversa ${\\boldsymbol A}^{-1}$ se esiste è unica.\nSupponiamo che la tesi non sia vera e procediamo per assurdo, ovvero ammettiamo che esista una matrice $\\hat{\\boldsymbol A}^{-1}\\neq{\\boldsymbol A}^{-1}$ tale che $\\hat{\\boldsymbol A}^{-1}{\\boldsymbol A}={\\boldsymbol I}_n$. Allora ${\\boldsymbol A}^{-1}={\\boldsymbol A}^{-1}{\\boldsymbol A}\\hat{\\boldsymbol A}^{-1}=\\hat{\\boldsymbol A}^{-1}$ che è l\u0026rsquo;assurdo cercato.\nDalla definizione, $({\\boldsymbol A}^{-1})^{-1}={\\boldsymbol A}$ e inoltre ${\\boldsymbol I}^{-1}_n={\\boldsymbol I}_n$. Infine, vale la seguente\nSiano ${\\boldsymbol A}\\in\\mathcal M_{n,p}({\\mathbb K})$ e ${\\boldsymbol B}\\in{\\mathbb K}^{p\\times m}$ entrambe dotate di inversa. Allora $({\\boldsymbol A}{\\boldsymbol B})^{-1}={\\boldsymbol B}^{-1}{\\boldsymbol A}^{-1}\\in\\mathcal M_{m,n}({\\mathbb K})$.\nIl sottoinsieme di $\\mathcal M_{n,n}({\\mathbb K})$ dato dalle matrici invertibili si denota $\\mathsf{GL}_{n}({\\mathbb K})$ ed ha la struttura di gruppo. Se ${\\mathbb K}={\\mathbb R}$, all\u0026rsquo;interno di tale sottinsieme si trova il sottinsieme $\\mathsf O_n({\\mathbb R})$ delle matrici ortogonali, ovvero delle matrici ${\\boldsymbol A}$ tali per cui ${\\boldsymbol A}^{-1}={\\boldsymbol A}^\\intercal$.\nConcludiamo questa sezione sulle matrici quadrate introducendo un\u0026rsquo;ultima importante quantità, ovvero il determinante di una matrice quadrata.\nData una matrice quadrata ${\\boldsymbol A}\\in\\mathcal M_{n,n}({\\mathbb K})$, il determinante di ${\\boldsymbol A}$ è uno scalare in ${\\mathbb K}$ definito ricorsivamente come\n$$\\det{\\boldsymbol A}=\\sum_{\\mu\\nu}(-1)^{\\mu+\\nu} a_{\\mu\\nu}\\det({\\boldsymbol A}_{\\mu\\nu})$$\ndove ${\\boldsymbol A}_{\\mu\\nu}\\in{\\mathbb K}^{(n-1)\\times(n-1)}$ è la matrice $(n-1)\\times (n-1)$ ottenuta da ${\\boldsymbol A}$ rimuovendo la $\\mu$ riga e la $\\nu$ colonna. Inoltre, dato uno scalare $c\\in{\\mathbb K}$, $\\det(c)=c$.\nLa definizione sopra, dovuta a Laplace, è ricorsiva ma permette di ottenere l\u0026rsquo;espressione del determinante in tutti i casi. Per esempio, per $n=2$\n$$\\det \\begin{pmatrix} a_{11}\u0026amp;a_{12}\\ a_{21}\u0026amp;a_{22} \\end{pmatrix}\n=a_{11}a_{22}-a_{12}a_{21}.$$\nNonostante la sua definizione apparentemente esotica, il determinante è, come espresso dal nome, cruciale per comprendere molte proprietà delle matrici quadrate. Esso stesso gode di alcune proprietà riassunte nel seguente\n[[t:LinAlg:det]]{#t:LinAlg:det label=\u0026ldquo;t:LinAlg:det\u0026rdquo;} Sia ${\\boldsymbol A}\\in\\mathcal M_{n\\times n}({\\mathbb K})$. Allora\nse ${\\boldsymbol A}$ è una matrice diagonale, allora $\\det{\\boldsymbol A}=\\prod_{i=1}^n a_{ii}$.\n$\\det{\\boldsymbol A}=\\det{\\boldsymbol A}^\\intercal$ (e di conseguenza proprietà riferite alle colonne si applicano anche alle righe).\nDato uno scalare $c\\in{\\mathbb K}$, $\\det (c{\\boldsymbol A}) = c^n \\det{\\boldsymbol A}$.\nSia una colonna di ${\\boldsymbol A}$ tale che ${\\boldsymbol a}^\\nu=\\lambda{\\boldsymbol v}+{\\boldsymbol u}$, i.e., $a_{\\mu\\nu}=\\lambda v_\\nu+u_\\nu$. Allora\n$$\\begin{gathered} \\det{\\boldsymbol A}=\\det \\left( \\begin{array}{cc\u0026gt;{\\columncolor{gray!40}}ccc} a_{11} \u0026amp; \\dots \u0026amp; a_{1\\nu} \u0026amp; \\dots \u0026amp; a_{1n} \\ \\vdots \u0026amp;\\vdots \u0026amp;\\vdots\u0026amp;\\vdots \u0026amp; \\vdots\\ a_{n1}\u0026amp;\\cdots \u0026amp;a_{n\\nu} \u0026amp;\\cdots \u0026amp;a_{nn} \\end{array}\n\\right)\\=\\lambda\\det \\left( \\begin{array}{cc\u0026gt;{\\columncolor{gray!40}}ccc} a_{11} \u0026amp; \\dots \u0026amp; v_{1} \u0026amp; \\cdots \u0026amp; a_{1n} \\ \\vdots \u0026amp;\\vdots \u0026amp;\\vdots\u0026amp;\\vdots \u0026amp; \\vdots\\ a_{n1}\u0026amp;\\cdots \u0026amp;v_{n} \u0026amp;\\cdots \u0026amp;a_{nn} \\end{array}\n\\right)+\\det \\left( \\begin{array}{cc\u0026gt;{\\columncolor{gray!40}}ccc} a_{11} \u0026amp; \\dots \u0026amp; u_{1} \u0026amp; \\dots \u0026amp; a_{1n} \\ \\vdots \u0026amp;\\vdots \u0026amp;\\vdots\u0026amp;\\vdots \u0026amp; \\vdots\\ a_{n1}\u0026amp;\\cdots \u0026amp;u_{n} \u0026amp;\\cdots \u0026amp;a_{nn} \\end{array}\n\\right)\\end{gathered} $$\nSi dice che di conseguenza il determinante è multilineare.\nSe ${\\boldsymbol A}$ ha due colonne identiche, $\\det{\\boldsymbol A}=0$.\nSia ${\\boldsymbol B}\\in\\mathcal M_{n\\times n}({\\mathbb K})$; allora $\\det {\\boldsymbol A}{\\boldsymbol B}=\\det{\\boldsymbol A}\\det {\\boldsymbol B}$.\nUna delle conseguenze delle proprietà suddette è che il determinante è alternante, ovvero guadagna un segno se due colonne vengono scambiate:\n$$\\begin{gathered} \\det \\left( \\begin{array}{cc\u0026gt;{\\columncolor{gray!20}}cc\u0026gt;{\\columncolor{gray!60}}ccc} a_{11} \u0026amp; \\dots \u0026amp; a_{1k} \u0026amp;\\dots \u0026amp; a_{1k\u0026rsquo;} \u0026amp; \\dots \u0026amp; a_{1n} \\ \\vdots \u0026amp;\\vdots \u0026amp;\\vdots\u0026amp;\\vdots \u0026amp; \\vdots\u0026amp;\\vdots \u0026amp; \\vdots\\ a_{n1}\u0026amp;\\cdots \u0026amp;a_{nk} \u0026amp;\\cdots \u0026amp;a_{nk\u0026rsquo;}\u0026amp;\\cdots \u0026amp; a_{nn} \\end{array}\n\\right)\\=-\\det \\left( \\begin{array}{cc\u0026gt;{\\columncolor{gray!60}}cc\u0026gt;{\\columncolor{gray!20}}ccc} a_{11} \u0026amp; \\dots \u0026amp; a_{1k\u0026rsquo;} \u0026amp;\\dots \u0026amp; a_{1k} \u0026amp; \\dots \u0026amp; a_{1n} \\ \\vdots \u0026amp;\\vdots \u0026amp;\\vdots\u0026amp;\\vdots \u0026amp; \\vdots\u0026amp;\\vdots \u0026amp; \\vdots\\ a_{n1}\u0026amp;\\cdots \u0026amp;a_{nk\u0026rsquo;} \u0026amp;\\cdots \u0026amp;a_{nk}\u0026amp;\\cdots \u0026amp; a_{nn} \\end{array}\n\\right)\\end{gathered} $$\nIl ruolo cruciale del determinante è legato anche al seguente lemma che caratterizza l\u0026rsquo;invertibilità delle matrici quadrate.\n[[l:LinAlg1:inv]]{#l:LinAlg1:inv label=\u0026ldquo;l:LinAlg1:inv\u0026rdquo;} Sia ${\\boldsymbol A}\\in\\mathcal M_{n\\times n}({\\mathbb K})$; sia ${\\boldsymbol C}$ la matrice dei suoi cofattori, ovvero la matrice $n\\times n$ con elementi\n$$c_{\\mu\\nu} = (-1)^{\\mu\\nu}\\det{\\boldsymbol A}_{\\mu\\nu}.$$ Allora, se $\\det{\\boldsymbol A}\\neq 0$, l\u0026rsquo;inversa di ${\\boldsymbol A}$ esiste ed è data da\n$${\\boldsymbol A}^{-1} = \\frac{1}{\\det{\\boldsymbol A}} {\\boldsymbol C}^\\intercal.$$\nSistemi di equazioni lineari Le definizioni date finora sono funzionali ad una serie di importanti applicazioni. La prima che andremo a considerare è la soluzione di sistemi lineari di equazioni, ovvero sistemi nella forma\n$$\\label{eq:linsyst}\n\\begin{cases} a_{11}x_1+a_{12}x_2+\\dots+a_{1n}x_n=b_1\\ a_{21}x_1+a_{22}x_2+\\dots+a_{2n}x_n=b_2\\ \\vdots\\ a_{m1}x_1+a_{m2}x_2+\\dots+a_{mn}x_n=b_m \\end{cases} $$\nin cui si assume che i coefficienti $a_{\\mu\\nu}\\in{\\mathbb K}$ e $b_\\nu\\in{\\mathbb K}$ siano noti e l\u0026rsquo;obiettivo è ottenere $x_\\nu\\in{\\mathbb K}$. Indicando con ${\\boldsymbol A}=(a_{\\mu\\nu}){\\mu\\nu}\\in\\mathcal M{m,n}({\\mathbb K})$, ${\\boldsymbol b}=(b_\\nu)\\nu\\in{\\mathbb K}^m$, ${\\boldsymbol x}=(x\\nu)_{\\nu}\\in{\\mathbb K}^n$, possiamo scrivere il sistema sopra in una forma molto più compatta come\n$${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}.$$\nIl sistema di equazioni è detto omogeneo se ${\\boldsymbol b}=\\mathbf 0$, mentre diversamente è detto non omogeneo. Ogni sistema è detto compatibile se ha almeno una soluzione: notare che un sistema omogeneo ha sempre la soluzione banale ${\\boldsymbol x}=\\mathbf 0$. Se è dato un sistema non omogeneo ${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}$, ${\\boldsymbol b}\\neq\\mathbf 0$, allora il sistema omogeneo ad esso associato è ${\\boldsymbol A}{\\boldsymbol x}=\\mathbf 0$.\nMetodo di eliminazione di Gauss Risolvere un sistema tipo ${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}$ è il problema centrale dell\u0026rsquo;algebra lineare. L\u0026rsquo;idea generale è utilizzare operazioni matriciali per poter infine ottenere una espressione per la soluzione ${\\boldsymbol x}$, se essa esiste. Un primo approccio che possiamo tentare è il cosiddetto metodo di eliminazione di Gauss. Supponiamo di avere un sistema come in Eq. [eq:linsyst]{reference-type=\u0026ldquo;eqref\u0026rdquo; reference=\u0026ldquo;eq:linsyst\u0026rdquo;}. Questo può essere associato alla seguente matrice\n$$\\begin{pmatrix} a_{11}\u0026amp;a_{12}\u0026amp;\\dots\u0026amp;a_{1n}\u0026amp;b_1\\\\ a_{21}\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{2n}\u0026amp;b_2\\\\ \\cdots\u0026amp;\\cdots\u0026amp;\\ddots\u0026amp;\\vdots\\\\ a_{m1}\u0026amp;a_{m2}\u0026amp;\\dots\u0026amp;a_{mn}\u0026amp;b_m \\end{pmatrix} $$\nottenuta concatenando la matrice ${\\boldsymbol A}$ e la colonna ${\\boldsymbol b}$ e talvolta detta matrice orlata. Ogni matrice siffatta corrisponde ad un sistema come in Eq. [eq:linsyst]{reference-type=\u0026ldquo;eqref\u0026rdquo; reference=\u0026ldquo;eq:linsyst\u0026rdquo;}. Possiamo eseguire una serie di operazioni di riga su questa matrice che lasciano inalterato il problema. In particolare\npossiamo moltiplicare una riga per una costante $c\\in{\\mathbb K}$ che sia non nulla;\npossiamo scambiare due righe;\npossiamo aggiungere ad una riga un multiplo di un\u0026rsquo;altra riga.\nQuesto tipo di operazioni può essere utilizzato per risolvere il sistema di equazioni lineari. Vediamo come con un esempio.\nCerchiamo di risolvere il sistema\n$$\\begin{cases} 2x_1+4x_2-2x_3=2\\ 4x_1+9x_2-3x_3=8\\ -2x_1-3x_2+7x_3=10 \\end{cases}\n\\Leftrightarrow\n\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\\ 4\u0026amp;9\u0026amp;-3\\ -2\u0026amp;-3\u0026amp;7 \\end{pmatrix}\n\\begin{pmatrix} x_1\\x_2\\x_3 \\end{pmatrix}\n= \\begin{pmatrix}2\\8\\10 \\end{pmatrix} $$\ncosì che la matrice orlata è\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 4\u0026amp;9\u0026amp;-3\u0026amp;8\\ -2\u0026amp;-3\u0026amp;7\u0026amp;10 \\end{pmatrix} $$\nProviamo ora a eseguire una operazione tale per cui la variabile $x_1$ venga rimossa da tutte le equazioni eccetto la prima. Ciò equivale a dire che gli elementi sotto $a_{11}$ devono essere trasformati in zero eseguendo una delle operazioni di riga indicate sopra. Per esempio, sostituiamo la $i$-esima riga con $(\\text{$i$ riga})-\\frac{a_{i1}}{a_{11}}(\\text{prima riga})$. L\u0026rsquo;elemento $a_{11}$ è detto primo pivot ed è cruciale che non sia zero. Se dovesse esserlo, basta scambiare le righe in maniera tale che compaia come prima una riga con $a_{11}\\neq 0$. Si ottiene quindi\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 0\u0026amp;1\u0026amp;1\u0026amp;4\\ 0\u0026amp;1\u0026amp;5\u0026amp;12 \\end{pmatrix} $$\nRipetiamo la procedura partendo dalla seconda riga verso il basso. Questa volta il pivot è $a_{22}=1$ e si procede come sopra per le righe $i\u0026gt;2$: $(\\text{$i$ riga})-\\frac{a_{i2}}{a_{22}}(\\text{prima riga})$, ottenendo\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 0\u0026amp;1\u0026amp;1\u0026amp;4\\ 0\u0026amp;0\u0026amp;4\u0026amp;8 \\end{pmatrix} $$\nQuesta matrice è a gradini e siamo arrivati alla forma desiderata. Perché questo è utile? Se scriviamo il sistema associato, esso è\n$$\\begin{cases} 2x_1+4x_2-2x_3=2\\ x_2+x_3=4\\ 4x_3=8 \\end{cases} $$\nche può essere risolto all\u0026rsquo;indietro, ovvero risolvendo prima per $x_3$, ottenendo $x_3=2$, la cui soluzione può essere sostituita nella seconda riga che dà $x_2=2$ e infine inserendo entrambe queste soluzioni nella prima, che fornisce $x_1=-1$.\nL\u0026rsquo;idea del metodo di eliminazione di Gauss consiste quindi nel trasformare la matrice orlata in una matrice a gradini che ammetta più facilmente una soluzione. Se $m\\leq n$, tale matrice finale avrà la forma\n$$\\begin{pmatrix} a_{11}\u0026amp;a_{12}\u0026amp;\\dots\u0026amp;a_{1m}\u0026amp;\\dots\u0026amp;a_{1n}\u0026amp;b_1\\ 0\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{2m}\u0026amp;\\dots\u0026amp;a_{2n}\u0026amp;b_2\\ \\vdots\u0026amp;\\vdots\u0026amp;\\ddots\u0026amp;\\cdots\u0026amp;\\cdots\u0026amp;\\vdots\u0026amp;\\vdots\\ 0\u0026amp;0\u0026amp;\\dots\u0026amp;a_{mm}\u0026amp;\\dots\u0026amp;a_{mn}\u0026amp;b_m \\end{pmatrix}\n.$$\nUna matrice a gradini è tale per cui, per ogni riga, il primo elemento non-nullo si trova a destra del primo elemento non-nullo della riga precedente. Supponiamo ora che $\\prod_{i=1}^ma_{ii}\\neq 0$. L\u0026rsquo;esempio mostra che, se $m=n$ e il nostro algoritmo ci fornisce la matrice orlata a gradini, allora il sistema è risolubile: l\u0026rsquo;ultima riga fornisce l\u0026rsquo;equazione per $x_n$, la penultima per $x_{n-1}$ e così via: in questo caso la soluzione del sistema è unica. Se invece $m\u0026lt;n$, l\u0026rsquo;ultima equazione è semplicemente\n$$a_{mm}x_m+\\dots+a_{mn}x_n=b_m$$ ovvero ci dice che la variabile $x_m$ può essere espressa in termini delle variabili $x_{m+1},\\dots,x_{n}$ come $x_m=\\frac{1}{a_{mm}}\\left(b_m-\\sum_{i=m+1}^na_{mi}x_i\\right)$. Non abbiamo sufficiente informazione per fissare le variabili $x_i$ con $m+1\\leq i\\leq n$ che peranto rimangono parametri arbitrari fissati i quali tutte le altre variabili possono essere fissate univocamente. Questo vuol dire che le soluzioni del nostro sistema vivono in uno spazio $n-m$ dimensionale.\nTuttavia il metodo può fallire se la matrice a gradini ottenuta ha $\\prod_{i=1}^ma_{ii}=0$ o se $m\u0026gt;n$.\nCerchiamo di risolvere il sistema\n$$\\begin{cases} 2x_1+4x_2-2x_3=2\\ 4x_1+8x_2-3x_3=8\\ -2x_1-4x_2+7x_3=10 \\end{cases}\n\\Leftrightarrow\n\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\\ 4\u0026amp;8\u0026amp;-3\\ -2\u0026amp;-4\u0026amp;7 \\end{pmatrix}\n\\begin{pmatrix} x_1\\x_2\\x_3 \\end{pmatrix}\n= \\begin{pmatrix}2\\8\\10 \\end{pmatrix} $$\ncosì che la matrice orlata è\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 4\u0026amp;8\u0026amp;-3\u0026amp;8\\ -2\u0026amp;-4\u0026amp;7\u0026amp;10 \\end{pmatrix} $$\nSeguiamo la procedura di Gauss. Al primo step otteniamo\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 0\u0026amp;0\u0026amp;1\u0026amp;4\\ 0\u0026amp;0\u0026amp;5\u0026amp;12 \\end{pmatrix} $$\nmentre al secondo\n$$\\begin{pmatrix} 2\u0026amp;4\u0026amp;-2\u0026amp;2\\ 0\u0026amp;0\u0026amp;1\u0026amp;4\\ 0\u0026amp;0\u0026amp;0\u0026amp;-8 \\end{pmatrix} $$\nL\u0026rsquo;ultima riga in particolare corrisponde ad una equazione falsa, ovvero $0=-8$. Questo significa che il sistema è incompatibile.\nSe $m\u0026gt;n$ la matrice orlata a gradini finale avrà una forma tipo\n$$\\begin{pmatrix} a_{11}\u0026amp;a_{12}\u0026amp;\\dots\u0026amp;a_{1m}\u0026amp;\\dots\u0026amp;a_{1n}\u0026amp;b_1\\ 0\u0026amp;a_{22}\u0026amp;\\dots\u0026amp;a_{2m}\u0026amp;\\dots\u0026amp;a_{2n}\u0026amp;b_2\\ \\vdots\u0026amp;\\vdots\u0026amp;\\ddots\u0026amp;\\cdots\u0026amp;\\cdots\u0026amp;\\vdots\u0026amp;\\vdots\\ 0\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;\\dots\u0026amp;a_{n,n}\u0026amp;b_{n}\\ 0\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;b_{n+1}\\ 0\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;\\vdots\\ 0\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;\\dots\u0026amp;0\u0026amp;b_{m}\\ \\end{pmatrix}\n.$$\nSe, per via della procedura, $b_{n+1}=\\dots=b_m=0$, allora possiamo trascurare tutte le righe dalla $(n+1)$esima in poi, dato che non sono informative, e ci riduciamo ad un caso in cui di fatto $m=n$. Diversamente, se uno dei valori $b_i\\neq0$ per $n+1\\leq i\\leq m$, allora la procedura è risultata in una condizione falsa, che vuol dire che il nostro sistema è incompatibile e non esistono soluzioni.\nIn conclusione, per $m\\leq n$, il sistema è compatibile se la matrice orlata può essere messa in una forma a gradini con $\\prod_{i=1}^ma_{ii}\\neq 0$. Se $n=m$ questa condizione è sufficiente a garantire che la soluzione è unica. In tutti gli altri casi, il sistema è incompatibile se la procedura genera equazioni nella forma $0=b_i$ con $b_i\\neq 0$.\nMetodo di Gauss\u0026ndash;Jordan per l\u0026rsquo;inversa Intuitivamente, il metodo di Gauss descritto sopra, quando ha successo, permette di \u0026ldquo;invertire\u0026rdquo; la matrice ${\\boldsymbol A}$ nell\u0026rsquo;equazione ${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}$ e, in particolare, ci aspettiamo che, se ${\\boldsymbol A}$ è una matrice quadrata invertibile, allora in effetti ${\\boldsymbol x}={\\boldsymbol A}^{-1}{\\boldsymbol b}$. In qualche modo, in questo caso, il metodo produce esattamente il risultato di questa operazione applicando l\u0026rsquo;inversa di ${\\boldsymbol A}$ a ${\\boldsymbol b}$, quando questa esiste.\nIn effetti, supponiamo che la matrice ${\\boldsymbol A}\\in\\mathcal M_{n,n}({\\mathbb K})$. La matrice inversa ${\\boldsymbol A}^{-1}$ è tale per cui ${\\boldsymbol A}{\\boldsymbol A}^{-1}={\\boldsymbol I}_n$. Se denoto con ${\\boldsymbol x}_i$ la $i$-esima colonna di ${\\boldsymbol A}^{-1}$\n$${\\boldsymbol A}{\\boldsymbol x}_i={\\boldsymbol e}i,\\qquad \\text{dove}\\quad {\\boldsymbol e}i=(\\delta{\\nu i}){\\nu}\\quad \\text{per }i=1,\\dots, n.$$\nIn sostanza trovare l\u0026rsquo;inversa equivale a risolvere contemporaneamente $n$ sistemi di equazioni lineari, problema per il quale possiamo applicare il metodo di Gauss. Per meglio esemplificare questo fatto, ricorriamo ad un esempio.\nConsideriamo la matrice quadrata\n$${\\boldsymbol A}= \\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\\ -1\u0026amp;2\u0026amp;-1\\ 0\u0026amp;-1\u0026amp;2\\ \\end{pmatrix}\n.$$ Per trovare l\u0026rsquo;inversa\n$${\\boldsymbol A}^{-1}= \\begin{pmatrix} x_{11}\u0026amp;x_{12}\u0026amp;x_{13}\\ x_{21}\u0026amp;x_{22}\u0026amp;x_{23}\\ x_{31}\u0026amp;x_{22}\u0026amp;x_{33}\\ \\end{pmatrix} $$ occorre risolvere le equazioni\n$$\\medmuskip=0mu \\thinmuskip=0mu \\thickmuskip=0mu \\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\\ -1\u0026amp;2\u0026amp;-1\\ 0\u0026amp;-1\u0026amp;2\\ \\end{pmatrix}\n\\begin{pmatrix} x_{11}\\x_{21}\\x_{31} \\end{pmatrix}\n= \\begin{pmatrix} 1\\0\\0 \\end{pmatrix}\n,\\qquad \\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\\ -1\u0026amp;2\u0026amp;-1\\ 0\u0026amp;-1\u0026amp;2\\ \\end{pmatrix}\n\\begin{pmatrix} x_{11}\\x_{21}\\x_{31} \\end{pmatrix}\n= \\begin{pmatrix} 0\\1\\0 \\end{pmatrix}\n,\\qquad \\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\\ -1\u0026amp;2\u0026amp;-1\\ 0\u0026amp;-1\u0026amp;2\\ \\end{pmatrix}\n\\begin{pmatrix} x_{11}\\x_{21}\\x_{31} \\end{pmatrix}\n= \\begin{pmatrix} 0\\0\\1 \\end{pmatrix}\n.$$ Possiamo immaginare di procedere parallelamente su tutti i sistemi usando una matrice orlata\n$$\\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ -1\u0026amp;2\u0026amp;-1\u0026amp;0\u0026amp;1\u0026amp;0\\ 0\u0026amp;-1\u0026amp;2\u0026amp;0\u0026amp;0\u0026amp;1\\ \\end{pmatrix} $$ Questa matrice orlata ha la forma\n$$\\begin{pmatrix} {\\boldsymbol A}\u0026amp;{\\boldsymbol I} \\end{pmatrix} $$ Usando la procedura di Gauss per mettere la matrice in forma a gradini si ottiene\n$$\\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;\\sfrac{3}{2}\u0026amp;-1\u0026amp;\\sfrac{1}{2}\u0026amp;1\u0026amp;0\\ 0\u0026amp;0\u0026amp;\\sfrac{4}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;1\\ \\end{pmatrix} $$ A questo punto vorremmo mettere il sistema nella forma\n$$\\begin{pmatrix} {\\boldsymbol I}\u0026amp;{\\boldsymbol B} \\end{pmatrix} $$ perché in questo caso ${\\boldsymbol B}={\\boldsymbol A}^{-1}$! La matrice orlata ottenuta infatti corrisponderebbe al set di sistemi $x_{\\mu\\nu}=b_{\\mu\\nu}$ fornendoci la soluzione che cerchiamo. Dobbiamo quindi lavorare per rimuovere gli zeri sopra la diagonale procedendo stavolta dal basso verso l\u0026rsquo;alto.\n$$\\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;\\sfrac{3}{2}\u0026amp;-1\u0026amp;\\sfrac{3}{4}\u0026amp;\\sfrac{3}{2}\u0026amp;\\sfrac{3}{4}\\ 0\u0026amp;0\u0026amp;\\sfrac{4}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;1 \\end{pmatrix}\n\\Rightarrow \\begin{pmatrix} 2\u0026amp;-1\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;\\sfrac{3}{2}\u0026amp;0\u0026amp;\\sfrac{3}{4}\u0026amp;\\sfrac{3}{2}\u0026amp;\\sfrac{3}{4}\\ 0\u0026amp;0\u0026amp;\\sfrac{4}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;1 \\end{pmatrix}\n\\Rightarrow \\begin{pmatrix} 2\u0026amp;0\u0026amp;0\u0026amp;\\sfrac{3}{2}\u0026amp;1\u0026amp;\\sfrac{1}{2}\\ 0\u0026amp;\\sfrac{3}{2}\u0026amp;0\u0026amp;\\sfrac{3}{4}\u0026amp;\\sfrac{3}{2}\u0026amp;\\sfrac{3}{4}\\ 0\u0026amp;0\u0026amp;\\sfrac{4}{3}\u0026amp;\\sfrac{1}{3}\u0026amp;\\sfrac{2}{3}\u0026amp;1 \\end{pmatrix}\n.$$ Dividendo opportunamente ogni riga otteniamo\n$$\\begin{pmatrix} 1\u0026amp;0\u0026amp;0\u0026amp;\\sfrac{3}{4}\u0026amp;\\sfrac{1}{2}\u0026amp;\\sfrac{1}{4}\\ 0\u0026amp;1\u0026amp;0\u0026amp;\\sfrac{1}{2}\u0026amp;1\u0026amp;\\sfrac{1}{2}\\ 0\u0026amp;0\u0026amp;1\u0026amp;\\sfrac{1}{4}\u0026amp;\\sfrac{1}{2}\u0026amp;\\sfrac{3}{4} \\end{pmatrix} $$ per cui\n$${\\boldsymbol A}^{-1}= \\begin{pmatrix} \\sfrac{3}{4}\u0026amp;\\sfrac{1}{2}\u0026amp;\\sfrac{1}{4}\\ \\sfrac{1}{2}\u0026amp;1\u0026amp;\\sfrac{1}{2}\\ \\sfrac{1}{4}\u0026amp;\\sfrac{1}{2}\u0026amp;\\sfrac{3}{4} \\end{pmatrix}\n.$$\nIl metodo descritto è detto di Gauss\u0026ndash;Jordan. La procedura fallisce se l\u0026rsquo;inversa cercata non esiste. Questo è il caso, per esempio, se dopo aver ottenuto la forma a gradini $\\prod_{i=1}^na_{ii}=0$. Questa condizione ha un significato preciso che specificheremo.\nRango Il metodo di eliminazione di Gauss permette di risolvere un sistema di equazioni lineari, o determinarne l\u0026rsquo;incompatibilità. Non fornisce però (almeno direttamente) un criterio di risolubilità né è chiaro perché in alcuni casi dovrebbe funzionare e in altri fallire. A questo scopo introduciamo il concetto di rango.\nDato un insieme finito di vettori ${{\\boldsymbol v}i}{i=1}^k$ di uno spazio vettoriale $\\mathbb V$, il rango dell\u0026rsquo;insieme è la dimensione di $\\mathrm{span}[{{\\boldsymbol v}i}{i=1}^k]$.\nData una matrice ${\\boldsymbol A}$ possiamo quindi assegnarle, in linea di principio, un rango per righe, ovvero il numero massimo di sue righe linearmenti indipendenti, e un rango per colonne, ovvero il numero massimo di sue colonne linearmenti indipendenti. Tuttavia, vale il seguente\nIl rango per righe e il rango per colonne di una matrice ${\\boldsymbol A}\\in{\\mathbb R}^{m\\times n}$ coincidono.\nDenotiamo d\u0026rsquo;ora in poi $r({\\boldsymbol A})$ il rango di ${\\boldsymbol A}$: naturalmente $r({\\boldsymbol A})\\leq \\min{n,m}$. Il rango di una matrice ha numerose proprietà, che non elencheremo, eccezion fatta per la seguente\nUna matrice quadrata ${\\boldsymbol A}\\in\\mathcal M_{n,n}({\\mathbb K})$ è invertibile se e solo se il suo rango è $n$.\nDi conseguenza, il rango di una matrice è massimo se e solo se il suo determinante è non nullo.\nIl rango di una matrice rimane inalterato eseguendo le opreazioni lineari del metodo di Gauss, ed è uguale al numero di righe non-nulle ottenute alla fine dell\u0026rsquo;esecuazione all\u0026rsquo;interno della matrice ${\\boldsymbol A}$ trasformata. Il fatto che il rango di una matrice sia legato alla risolubilità di un sistema lineare è suggerito dal seguente fatto. Consideriamo il problema ${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}$: se indichiamo con ${\\boldsymbol a}^\\nu$ la $\\mu$-esima colonna di ${\\boldsymbol A}$, questo problema può scriversi come\n$$\\sum_{\\nu=1}^n x_\\nu{\\boldsymbol a}^\\nu={\\boldsymbol b},$$\nche esprime il fatto che stiamo cercando di scrivere ${\\boldsymbol b}$ come sovrapposizione lineare dei vettori colonna di ${\\boldsymbol A}$, ${{\\boldsymbol a}^\\nu}{\\nu=1}^n$. Ciò sarà possibile se ${\\boldsymbol b}$ vive nello spazio generato dalle colonne di ${\\boldsymbol A}$, $\\mathrm{span}[{{\\boldsymbol a}^\\nu}{\\nu=1}^n]$, la cui dimensione è $r({\\boldsymbol A})$. Questa intuizione è espressa in termini rigorosi da un teorema fondamentale dell\u0026rsquo;algebra lineare, che diamo senza dimostrazione.\nUn sistema lineare di $m$ equazioni in $n$ incognite ${\\boldsymbol A}{\\boldsymbol x}={\\boldsymbol b}$ è compatibile se e solo se la matrice orlata ha lo stesso rango della matrice ${\\boldsymbol A}$. In tal caso, lo spazio delle soluzioni ha dimensione $n-r({\\boldsymbol A})$.\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"4a23eaf42fd45fdff6ff78eb205d5d73","permalink":"https://gsicuro.github.io/lectures/im2on/ch1/sec2/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/lectures/im2on/ch1/sec2/","section":"lectures","summary":"Matrici Un esempio particolarmente importante e prototipico di spazio vettoriale è costituito dallo spazio delle matrici di elementi in ${\\mathbb K}$. Consideriamo ora due interi positivi $m$ e $n$. Una matrice di $m\\times n$ elementi in ${\\mathbb K}$ è una tabella del tipo","tags":null,"title":"Matrici","type":"book"},{"authors":null,"categories":null,"content":"Content\u0026hellip;\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"3e0448d5ddaa7f4e2c87dc40e5e39e96","permalink":"https://gsicuro.github.io/lectures/im2on/ch1/sec3/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/lectures/im2on/ch1/sec3/","section":"lectures","summary":"Content\u0026hellip;","tags":null,"title":"Sistemi di equazioni lineari","type":"book"},{"authors":null,"categories":null,"content":"Content\u0026hellip;\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"c83d30e0818ee80ae4765b05027267dd","permalink":"https://gsicuro.github.io/lectures/im2on/ch1/sec4/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/lectures/im2on/ch1/sec4/","section":"lectures","summary":"Content\u0026hellip;","tags":null,"title":"Operatori lineari e spettri","type":"book"},{"authors":["Gabriele Sicuro"],"categories":null,"content":"Materiale di riferimento Il testo di riferimento sarà il volume di Paolo Biscari, Tommaso Ruggeri, Giuseppe Saccomandi e Maurizio Vianello Meccanica Razionale, edito da Springer. Un utile manuale di esercizi è la raccolta di Francesca Brini, Augusto Muracchini, Tommaso Ruggeri e Leonardo Seccia Esercizi e Temi d\u0026rsquo;Esame di Meccanica Razionale pubblicato da Esculapio.\nNote del corso. Esame L\u0026rsquo;esame consiste di una prova scritta e una successiva prova orale opzionale.\nLa data della prova orale viene annunciata contestualmente all\u0026rsquo;appello (tipicamente entri 15 giorni dalla prova scritta). L\u0026rsquo;ammissione all\u0026rsquo;orale è subordinata al raggiungimento di un punteggio non inferiore a 15/30 nella prova scritta. L\u0026rsquo;intenzione di partecipare alla prova orale va comunicata al più entro tre giorni dalla pubblicazione dei risultati della prova scritta. Tutti gli studenti interessati a visionare la correzione della loro prova scritta possono presentarsi a questo scopo nella sede della prova orale associata all\u0026rsquo;appello, indipendentemente dal fatto che debbano o meno sostenere l\u0026rsquo;orale. L\u0026rsquo;iscrizione ad ogni appello ordinario si apre due mesi prima della sua pubblicazione e si chiude categoricamente una settimana prima dello stesso: non sono ammesse eccezionali iscrizioni ritardatarie. Voti dell'esame del modulo Meccanica Razionale del corso di laurea in Architettura--Ingegneria dell'Università di Bologna in tutti gli esami scritti dall'autunno 2023 (i ritiri vengono contati come voto zero). La curva associa all'ascissa x la frazione di studenti con voto maggiore di x. Approssimativamente il 40% degli studenti raggiunge la sufficienza. Prove passate Le prove sono riportate con relativa soluzione.\nProva esempio\n2024 09-09-2024 15-07-2024 01-07-2024 17-06-2024 22-03-2024 07-02-2024 17-01-2024 2023 01-12-2023 ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"beece67f19fe8ae65be7940f12dbfef4","permalink":"https://gsicuro.github.io/lectures/mr/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/lectures/mr/","section":"lectures","summary":"Modulo di *Meccanica Razionale* per il corso di laurea in Architettura--Ingegneria dell'Università di Bologna. [**Italian**]","tags":null,"title":"Meccanica Razionale","type":"lectures"},{"authors":["Gabriele Sicuro"],"categories":null,"content":" Materiale per il modulo di Istituzioni di Matematica 2 del corso di laurea in Architettura-Ingegneria dell'Università di Bologna durante l'anno accademico 2023-2024. Dispense Il corso consiste di due parti. La prima si focalizza su elementi dell\u0026rsquo;algebra lineare, mentre la seconda tratta i fondamenti del calcolo in molte variabili.\nNote del corso. Si noti che le note possono contenere errori. Esame Prova scritta e successiva prova orale; l\u0026rsquo;ammissione all\u0026rsquo;orale è subordinata al raggiungimento di un punteggio non inferiore a 15/30 nella prova scritta.\nProve passate Le prove sono riportate con relativa soluzione.\n10-09-2024 02-07-2024 18-06-2024 12-02-2024 31-01-2024 15-01-2024 ","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"a0e489f2492b2720605bd36bfcce6bbc","permalink":"https://gsicuro.github.io/lectures/im2/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/lectures/im2/","section":"lectures","summary":"Modulo di *Istituzioni di Matematica 2* per il corso di laurea in Architettura--Ingegneria dell'Università di Bologna. [**Italian**]","tags":null,"title":"Istituzioni di Matematica 2","type":"lectures"},{"authors":["gs"],"categories":null,"content":"","date":1694908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"97993091e93f95af1a5f15747b6f0276","permalink":"https://gsicuro.github.io/news/dynparis2023/","publishdate":"2023-09-17T00:00:00Z","relpermalink":"/news/dynparis2023/","section":"news","summary":"A workshop in Paris to bring together researchers for an informal and open discussion on the most promising research directions in neural network dynamics. Organised with [Bruno Loureiro](https://brloureiro.github.io/) and Stefano Sarao Mannelli.","tags":["Conferences"],"title":"Analytical Approaches ​for Neural Network Dynamics","type":"news"},{"authors":null,"categories":null,"content":" ","date":1694649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694649600,"objectID":"23d1e75f872528fc12f5f2b142375ff7","permalink":"https://gsicuro.github.io/publications/","publishdate":"2023-09-14T00:00:00Z","relpermalink":"/publications/","section":"","summary":"A list of","tags":null,"title":"Publications","type":"page"},{"authors":["gs"],"categories":null,"content":"","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"f54bf7fd0510b91b0dd95f2cb18f451d","permalink":"https://gsicuro.github.io/news/kuhn2023/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/news/kuhn2023/","section":"news","summary":"The KCL Disordered Systems group organised a [two-day event](https://disorderdayskcl.weebly.com/) on the physics of disorder. The first day of the event has been dedicated to our dear colleague **Reimer Kühn**, who retired in 2023.","tags":["Conferences"],"title":"Disordered Systems Days at King's College London","type":"news"},{"authors":["gs"],"categories":null,"content":"","date":1688119200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688119200,"objectID":"e30343b384b2257140fc969e0f3dbd9f","permalink":"https://gsicuro.github.io/news/rsb40/","publishdate":"2023-06-30T10:00:00Z","relpermalink":"/news/rsb40/","section":"news","summary":"An encyclopaedic overview of the broad range of the applications stemming from the discovery of the replica symmetry breaking solution of the Sherrington-Kirkpatrick model by Giorgio Parisi.","tags":["Nobel prize","Disordered Systems","Book","Reviews"],"title":"Spin Glass Theory \u0026 Far Beyond","type":"news"},{"authors":["Gabriele Sicuro"],"categories":null,"content":"These lecture notes are largely based on a previous version prepared by A. Annibale, for the Dynamical analysis of complex systems, later named Stochastic processes and applications, module at King\u0026rsquo;s College London. The module is part of the Complex Systems Modelling master and I taught it from 2021 to 2023.\n","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"4327a01af0295e0edbc4e251be574dc6","permalink":"https://gsicuro.github.io/lectures/ps/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/lectures/ps/","section":"lectures","summary":"A *Stochastic Processes* module for the Master in Complex Systems Modelling at King's College London in 2023.","tags":null,"title":"Stochastic Processes and applications","type":"lectures"},{"authors":["gs"],"categories":null,"content":"","date":1665964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666137600,"objectID":"21d5ea42bc08e7c60633855f52aafa6c","permalink":"https://gsicuro.github.io/news/caracciolo2022/","publishdate":"2022-10-17T00:00:00Z","relpermalink":"/news/caracciolo2022/","section":"news","summary":"A conference on Statistical Field Theory, Statistical Physics and their interdisciplinary applications in Trieste, in honour of Sergio Caracciolo's 70th birthday.","tags":["Conferences"],"title":"The Facets of Statistical Field Theory","type":"news"},{"authors":["gs"],"categories":null,"content":"","date":1654992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654992000,"objectID":"19e31efd2a6125035fbc252a2f70428f","permalink":"https://gsicuro.github.io/news/meco47/","publishdate":"2022-06-12T00:00:00Z","relpermalink":"/news/meco47/","section":"news","summary":"The traditional Conference of the Middle European Cooperation in Statistical Physics was organised in 2022 by Silvia Bartolucci, Laura Foini, Pierpaolo Vivo and myself in Erice.","tags":["Conferences"],"title":"MECO47","type":"news"},{"authors":["gs"],"categories":null,"content":"","date":1654473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654473600,"objectID":"1d0828d67920714e1d16748feab9026e","permalink":"https://gsicuro.github.io/news/bmc2022/","publishdate":"2022-06-06T00:00:00Z","relpermalink":"/news/bmc2022/","section":"news","summary":"The British Mathematical Colloquium has been hosted in 2022 at King's College London, and included a special session on complex systems with a list of prestigious guests including Andrea Montanari and Vittoria Colizza.","tags":["Conferences"],"title":"73rd British Mathematical Colloquium","type":"news"},{"authors":null,"categories":null,"content":"PhD students From 2021, Urte Adomaityte. King\u0026rsquo;s College London. Master students 2016, Matteo P. D\u0026rsquo;Achille. Thesis: On two assignment problems, co-supervised with Sergio Caracciolo, University of Milan. 2018, Gianmarco Perrupato. Thesis: Study of matching on the Bethe lattice, co-supervised with Giorgio Parisi, Sapienza University of Rome. 2021, Anshul Toshniwal. Thesis: The planted multi-index matching problem, co-supervised with Lenka Zdeborová, EPFL. 2021, Claudia De Sousa Miranda Perez. Thesis: The random dimer covering problem on the weighted Aztec graph, King\u0026rsquo;s College London. 2021, Daniel Reti. Thesis: Robustness of excitations in the random dimer model, King\u0026rsquo;s College London. 2021, Hugo Ryder. Thesis: Matching recovery, King\u0026rsquo;s College London. 2021, Leonardo Scialo. Thesis: Arctic region in the weighted random dimer covering, King\u0026rsquo;s College London. 2022, Guoyu Chang. Thesis: The mixed multi-index matching problem, King\u0026rsquo;s College London. 2022, Xiaoying Zhou. Thesis: Statistical physics of community detection, King\u0026rsquo;s College London. 2023, Wenjuan Li. Thesis: Stable Marriage Problems: Perturbations and Correlations, King\u0026rsquo;s College London. ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"c61bd17da54e6598c71ab43046ec8671","permalink":"https://gsicuro.github.io/alumni/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/alumni/","section":"","summary":"PhD students From 2021, Urte Adomaityte. King\u0026rsquo;s College London. Master students 2016, Matteo P. D\u0026rsquo;Achille. Thesis: On two assignment problems, co-supervised with Sergio Caracciolo, University of Milan. 2018, Gianmarco Perrupato. Thesis: Study of matching on the Bethe lattice, co-supervised with Giorgio Parisi, Sapienza University of Rome.","tags":null,"title":"Current and past students","type":"page"},{"authors":["gs"],"categories":null,"content":"","date":1568073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694822400,"objectID":"a9a1ea1bd1c34c35d17963613f9f1c7e","permalink":"https://gsicuro.github.io/news/confrsb40/","publishdate":"2019-09-10T00:00:00Z","relpermalink":"/news/confrsb40/","section":"news","summary":"A large event on 40th anniversary of the discovery of Replica Symmetry Breaking brings together in Rome researchers who developed, refined and applied RSB from a broad cross-section of the theoretical community.","tags":["Conferences"],"title":"40 years of Replica Symmetry Breaking","type":"news"},{"authors":["gs"],"categories":null,"content":"","date":1537315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694822400,"objectID":"0309af90e48cd616b6c2030b53fd7ace","permalink":"https://gsicuro.github.io/news/parisi70/","publishdate":"2018-09-19T00:00:00Z","relpermalink":"/news/parisi70/","section":"news","summary":"A conference at Sapienza University of Rome to celebrate the outstanding scientific achievement of Giorgio Parisi in occasion of his 70th birthday.","tags":["Conferences"],"title":"Disordered serendipity","type":"news"},{"authors":["gs"],"categories":null,"content":"","date":1514937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694822400,"objectID":"7a6cfda506eee987e135aa8824b31db7","permalink":"https://gsicuro.github.io/news/bmft/","publishdate":"2018-01-03T00:00:00Z","relpermalink":"/news/bmft/","section":"news","summary":"A three day workshop in Rome dedicated to Cirano De Dominicis, pioneer in the field theoretical approach to Spin Glasses.","tags":["Conferences"],"title":"Beyond mean field theory","type":"news"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://gsicuro.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]